{"id":222421400,"title":"Being bored? Recognising natural interest by extensive audiovisual integration for real-life application","authors":["Björn Schuller","Ronald Müller","Florian Eyben","Jürgen Gast","Benedikt Hörnler","Martin Wöllmer"],"_abstract":"ABSTRACT Automatic detection of the level of human interest is of high relevance for many technical applications, such as automatic customer care or tutoring systems. However, the recognition of spontaneous interest in natural conversations independently of the subject remains a challenge. Identification of human affective states relying on single modalities only is often impossible, even for humans, since different modalities contain partially disjunctive cues. Multimodal approaches to human affect recognition generally are shown to boost recognition performance, yet are evaluated in restrictive laboratory settings only. Herein we introduce a fully automatic processing combination of Active–Appearance–Model-based facial expression, vision-based eye-activity estimation, acoustic features, linguistic analysis, non-linguistic vocalisations, and temporal context information in an early feature fusion process. We provide detailed subject-independent results for classification and regression of the Level of Interest using Support-Vector Machines on an audiovisual interest corpus (AVIC) consisting of spontaneous, conversational speech demonstrating “theoretical” effectiveness of the approach. Further, to evaluate the approach with regards to real-life usability a user-study is conducted for proof of “practical” effectiveness.","cited_in":[{"id":282211304,"url":"https://www.researchgate.net/publication/282211304_Multimodal_Data_Collection_of_Human-Robot_Humorous_Interactions_in_the_JOKER_Project"},{"id":265013224,"url":"https://www.researchgate.net/publication/265013224_Detecting_Summarization_Hot_Spots_in_Meetings_Using_Group_Level_Involvement_and_Turn-Taking_Features"},{"id":265013144,"url":"https://www.researchgate.net/publication/265013144_Modelling_Participant_Affect_in_Meetings_with_Turn-Taking_Features"},{"id":237148069,"url":"https://www.researchgate.net/publication/237148069_Affective_State_Level_Recognition_in_Naturalistic_Facial_and_Vocal_Expressions"},{"id":257093326,"url":"https://www.researchgate.net/publication/257093326_The_MAHNOB_Laughter_database"},{"id":257093268,"url":"https://www.researchgate.net/publication/257093268_Categorical_and_dimensional_affect_analysis_in_continuous_input_Current_trends_and_future_directions"},{"id":267328035,"url":"https://www.researchgate.net/publication/267328035_Amplitude_Modulation_Features_for_Emotion_Recognition_from_Speech"},{"id":257571910,"url":"https://www.researchgate.net/publication/257571910_Synthesized_speech_for_model_training_in_cross-corpus_recognition_of_human_emotion"},{"id":262425538,"url":"https://www.researchgate.net/publication/262425538_Intrinsic_and_Extrinsic_Evaluation_of_an_Automatic_User_Disengagement_Detector_for_an_Uncertainty-adaptive_Spoken_Dialogue_System"},{"id":228513346,"url":"https://www.researchgate.net/publication/228513346_Speaker-independent_emotion_recognition_exploiting_a_psychologically-inspired_binary_cascade_classification_schema"},{"id":224929669,"url":"https://www.researchgate.net/publication/224929669_Audiovisual_Vocal_Outburst_Classification_in_Noisy_Acoustic_Conditions"},{"id":261091057,"url":"https://www.researchgate.net/publication/261091057_Recognizing_emotions_of_characters_in_movies"},{"id":220120511,"url":"https://www.researchgate.net/publication/220120511_Emotional_states_in_judicial_courtrooms_An_experimental_investigation"},{"id":224929626,"url":"https://www.researchgate.net/publication/224929626_Unsupervised_Learning_in_Cross-Corpus_Acoustic_Emotion_Recognition"},{"id":224929577,"url":"https://www.researchgate.net/publication/224929577_Affective_Speaker_State_Analysis_in_the_Presence_of_Reverberation"},{"id":220733712,"url":"https://www.researchgate.net/publication/220733712_Deep_neural_networks_for_acoustic_emotion_recognition_Raising_the_benchmarks"},{"id":220735544,"url":"https://www.researchgate.net/publication/220735544_Localization_of_non-linguistic_events_in_spontaneous_speech_by_Non-Negative_Matrix_Factorization_and_Long_Short-Term_Memory"},{"id":220395367,"url":"https://www.researchgate.net/publication/220395367_Real-Time_Recognition_of_Affective_States_from_Nonverbal_Features_of_Speech_and_Its_Application_for_Public_Speaking_Skill_Analysis"},{"id":221292323,"url":"https://www.researchgate.net/publication/221292323_String-based_audiovisual_fusion_of_behavioural_events_for_the_assessment_of_dimensional_affect"},{"id":224929672,"url":"https://www.researchgate.net/publication/224929672_Selecting_Training_Data_for_Cross-Corpus_Speech_Emotion_Recognition_Prototypicality_vs_Generalization"},{"id":224929625,"url":"https://www.researchgate.net/publication/224929625_Using_Multiple_Databases_for_Training_in_Emotion_Recognition_To_Unite_or_to_Vote"},{"id":224155355,"url":"https://www.researchgate.net/publication/224155355_Combining_Long_Short-Term_Memory_and_Dynamic_Bayesian_Networks_for_Incremental_Emotion-Sensitive_Artificial_Listening"},{"id":232638637,"url":"https://www.researchgate.net/publication/232638637_Cross-Corpus_Acoustic_Emotion_Recognition_Variances_and_Strategies"},{"id":220045150,"url":"https://www.researchgate.net/publication/220045150_Speaker-independent_negative_emotion_recognition"},{"id":224149743,"url":"https://www.researchgate.net/publication/224149743_Discrimination_of_speech_and_non-linguistic_vocalizations_by_Non-Negative_Matrix_Factorization"},{"id":41452754,"url":"https://www.researchgate.net/publication/41452754_Visual_and_Multimodal_Analysis_of_Human_Spontaneous_Behavior_Introduction_to_the_Special_Issue_of_Image_Vision_Computing_Journal"},{"id":228891935,"url":"https://www.researchgate.net/publication/228891935_String-based_Audiovisual_Fusion_of_Behavioural_Events_for_the_Assessment_of_Dimensional_Affect"},{"id":254706224,"url":"https://www.researchgate.net/publication/254706224_Microexpression_spotting_in_video_using_optical_strain"},{"id":221487336,"url":"https://www.researchgate.net/publication/221487336_Data-driven_clustering_in_emotional_space_for_affect_recognition_using_discriminatively_trained_LSTM_networks"},{"id":41387085,"url":"https://www.researchgate.net/publication/41387085_Implicit_Human_Centered_Tagging"},{"id":221486128,"url":"https://www.researchgate.net/publication/221486128_Recognising_interest_in_conversational_speech_-_Comparing_bag_of_frames_and_supra-segmental_features"},{"id":235779351,"url":"https://www.researchgate.net/publication/235779351_Spotting_Agreement_and_Disagreement_A_Survey_of_Nonverbal_Audiovisual_Cues_and_Tools"},{"id":224088060,"url":"https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit"},{"id":224088137,"url":"https://www.researchgate.net/publication/224088137_The_hinterland_of_emotions_Facing_the_open-microphone_challenge"},{"id":224586598,"url":"https://www.researchgate.net/publication/224586598_Multiscale_Genomic_Imaging_Informatics"},{"id":224586599,"url":"https://www.researchgate.net/publication/224586599_Implicit_Human-Centered_Tagging"},{"id":220552674,"url":"https://www.researchgate.net/publication/220552674_A_multidimensional_dynamic_time_warping_algorithm_for_efficient_multimodal_fusion_of_asynchronous_data_streams"},{"id":221481381,"url":"https://www.researchgate.net/publication/221481381_The_INTERSPEECH_2010_paralinguistic_challenge"},{"id":43170331,"url":"https://www.researchgate.net/publication/43170331_Determination_of_Nonprototypical_Valence_and_Arousal_in_Popular_Music_Features_and_Performances"},{"id":220316619,"url":"https://www.researchgate.net/publication/220316619_Emotion_on_the_Road-Necessity_Acceptance_and_Feasibility_of_Affective_Computing_in_the_Car"},{"id":50888741,"url":"https://www.researchgate.net/publication/50888741_Pitch_in_Emotional_Speech_and_Emotional_Speech_Recognition_Using_Pitch_Frequency"},{"id":224929667,"url":"https://www.researchgate.net/publication/224929667_A_Multimodal_Listener_Behaviour_Driven_by_Audio_Input"},{"id":221478005,"url":"https://www.researchgate.net/publication/221478005_Level_of_interest_sensing_in_spoken_dialog_using_multi-level_fusion_of_acoustic_and_lexical_evidence"},{"id":220395374,"url":"https://www.researchgate.net/publication/220395374_Cross-Corpus_Acoustic_Emotion_Recognition_Variances_and_Strategies"},{"id":281042381,"url":"https://www.researchgate.net/publication/281042381_Towards_a_standard_set_of_acoustic_features_for_the_processing_of_emotion_in_speech"},{"id":224096783,"url":"https://www.researchgate.net/publication/224096783_Acoustic_emotion_recognition_A_benchmark_comparison_of_performances"},{"id":224929634,"url":"https://www.researchgate.net/publication/224929634_On-line_Emotion_Recognition_in_a_3-D_Activation-Valence-Time_Continuum_using_Acoustic_and_Linguistic_Cues"},{"id":42386822,"url":"https://www.researchgate.net/publication/42386822_Segmenting_into_Adequate_Units_for_Automatic_Recognition_of_Emotion-Related_Episodes_A_Speech-Based_Approach"},{"id":220340310,"url":"https://www.researchgate.net/publication/220340310_Bidirectional_LSTM_Networks_for_Context-Sensitive_Keyword_Detection_in_a_Cognitive_Virtual_Agent_Framework"},{"id":226277763,"url":"https://www.researchgate.net/publication/226277763_Continuous_Analysis_of_Affect_from_Voice_and_Face"},{"id":252012056,"url":"https://www.researchgate.net/publication/252012056_Turkish_emotional_speech_database"},{"id":224929599,"url":"https://www.researchgate.net/publication/224929599_The_Automatic_Recognition_of_Emotions_in_Speech"},{"id":224929562,"url":"https://www.researchgate.net/publication/224929562_Voice_and_Speech_Analysis_in_Search_of_States_and_Traits"},{"id":47737865,"url":"https://www.researchgate.net/publication/47737865_Social_Network_Analysis_for_Automatic_Role_Recognition"},{"id":224929632,"url":"https://www.researchgate.net/publication/224929632_Acoustic-Linguistic_Recognition_of_Interest_in_Speech_with_Bottleneck-BLSTM_Nets"},{"id":224246552,"url":"https://www.researchgate.net/publication/224246552_Audiovisual_classification_of_vocal_outbursts_in_human_conversation_using_Long-Short-Term_Memory_networks"},{"id":226387747,"url":"https://www.researchgate.net/publication/226387747_Computational_Assessment_of_Interest_in_Speech-Facing_the_Real-Life_Challenge"},{"id":224262180,"url":"https://www.researchgate.net/publication/224262180_Building_Autonomous_Sensitive_Artificial_Listeners"},{"id":222650661,"url":"https://www.researchgate.net/publication/222650661_Recognising_realistic_emotions_and_affect_in_speech_State_of_the_art_and_lessons_learnt_from_the_first_challenge"},{"id":224248861,"url":"https://www.researchgate.net/publication/224248861_Recognizing_Affect_from_Linguistic_Information_in_3D_Continuous_Space"},{"id":224929559,"url":"https://www.researchgate.net/publication/224929559_Ten_Recent_Trends_in_Computational_Paralinguistics"},{"id":257513035,"url":"https://www.researchgate.net/publication/257513035_Features_and_classifiers_for_emotion_recognition_from_speech_a_survey_from_2000_to_2011"},{"id":224929590,"url":"https://www.researchgate.net/publication/224929590_A_Multi-Task_Approach_to_Continuous_Five-Dimensional_Affect_Sensing_in_Natural_Speech"},{"id":235899186,"url":"https://www.researchgate.net/publication/235899186_Semi-supervised_context_adaptation_Case_study_of_audience_excitement_recognition"},{"id":234112828,"url":"https://www.researchgate.net/publication/234112828_Towards_the_automatic_detection_of_spontaneous_agreement_and_disagreement_based_on_nonverbal_behavior_A_survey_of_related_cues_databases_and_tools"},{"id":257518433,"url":"https://www.researchgate.net/publication/257518433_Mandarin_emotion_recognition_combining_acoustic_and_emotional_point_information"},{"id":257470335,"url":"https://www.researchgate.net/publication/257470335_Speech_emotion_recognition_Features_and_classification_models"},{"id":233999161,"url":"https://www.researchgate.net/publication/233999161_Automatic_classification_of_eye_activity_for_cognitive_load_measurement_with_emotion_interference"},{"id":259065800,"url":"https://www.researchgate.net/publication/259065800_Modeling_phonetic_pattern_variability_in_favor_of_the_creation_of_robust_emotion_classifiers_for_real-life_applications"},{"id":224929574,"url":"https://www.researchgate.net/publication/224929574_Paralinguistics_in_Speech_and_Language_-_State-of-the-Art_and_the_Challenge"},{"id":224929589,"url":"https://www.researchgate.net/publication/224929589_LSTM-Modeling_of_Continuous_Emotions_in_an_Audiovisual_Affect_Recognition_Framework"},{"id":259889785,"url":"https://www.researchgate.net/publication/259889785_Sparse_Autoencoder-Based_Feature_Transfer_Learning_for_Speech_Emotion_Recognition"},{"id":256763314,"url":"https://www.researchgate.net/publication/256763314_Speech_Emotional_Features_Extraction_Based_on_Electroglottograph"},{"id":257652279,"url":"https://www.researchgate.net/publication/257652279_NovA_Automated_Analysis_of_Nonverbal_Signals_in_Social_Interactions"},{"id":262166224,"url":"https://www.researchgate.net/publication/262166224_Building_a_Recognition_System_of_Speech_Emotion_and_Emotional_States"},{"id":269295038,"url":"https://www.researchgate.net/publication/269295038_Robust_Canonical_Correlation_Analysis_Audio-visual_fusion_for_learning_continuous_interest"},{"id":269295622,"url":"https://www.researchgate.net/publication/269295622_Single-channel_speech_separation_with_memory-enhanced_recurrent_neural_networks"},{"id":265035288,"url":"https://www.researchgate.net/publication/265035288_Investigation_of_Speaker_Group-Dependent_Modelling_for_Recognition_of_Affective_States_from_Speech"},{"id":276026141,"url":"https://www.researchgate.net/publication/276026141_An_Overview_of_Research_on_Facial_Aging_using_the_FG-NET_Aging_Database"},{"id":281241468,"url":"https://www.researchgate.net/publication/281241468_Prediction-based_Audiovisual_Fusion_for_Classification_of_Non-Linguistic_Vocalisations"},{"id":282964996,"url":"https://www.researchgate.net/publication/282964996_Lightweight_Adaptation_of_Classifiers_to_Users_and_Contexts_Trends_of_the_Emerging_Domain"}],"reference":["https://www.researchgate.net/publication/3321357_Emotion_recognition_in_human-computer_interaction_Signal_Process_Mag","https://www.researchgate.net/publication/3703219_The_FERET_Evaluation_Methodology_for_Face_Recognition_Algorithm","https://www.researchgate.net/publication/239223250_The_Handbook_of_Cognition_and_Emotion","https://www.researchgate.net/publication/226022152_On_Usability_and_Usability_Engineering","https://www.researchgate.net/publication/220688404_Prosody_in_Speech_Understanding_Systems","https://www.researchgate.net/publication/2931099_Statistical_Models_of_Appearance_for_computer_vision","https://www.researchgate.net/publication/221265642_Efficient_Recognition_of_Authentic_Dynamic_Facial_Expressions_on_the_Feedtum_Database","https://www.researchgate.net/publication/228962487_Private_emotions_vs_social_interaction-towards_new_dimensions_in_research_on_emotion","https://www.researchgate.net/publication/224711604_Towards_More_Reality_in_the_Recognition_of_Emotional_Speech","https://www.researchgate.net/publication/224929635_Aspects_of_Efficient_Usability_Engineering","https://www.researchgate.net/publication/221786197_Machine_Analysis_of_Facial_Expressions","https://www.researchgate.net/publication/220726872_Static_and_Dynamic_Modelling_for_the_Recognition_of_Non-verbal_Vocalisations_in_Conversational_Speech","https://www.researchgate.net/publication/3424011_Multimodal_Integration_-_A_Statistical_View","https://www.researchgate.net/publication/3548274_Some_statistical_issues_in_the_comparison_of_speech_recognition_algorithms","https://www.researchgate.net/publication/224711182_Audiovisual_Behavior_Modeling_by_Combined_Feature_Spaces","https://www.researchgate.net/publication/221491576_Combining_acoustic_and_language_information_for_emotion_recognition","https://www.researchgate.net/publication/221259515_A_Comparative_Evaluation_of_Active_Appearance_Model_Algorithms","https://www.researchgate.net/publication/221052336_Audiovisual_recognition_of_spontaneous_interest_within_conversations","https://www.researchgate.net/publication/228616884_Multimodal_emotion_recognition","https://www.researchgate.net/publication/224680015_Multimodal_Face_Detection_Head_Orientation_and_Eye_Gaze_Tracking","https://www.researchgate.net/publication/221415729_Low-Level_Fusion_of_Audio_Video_Feature_for_Multi-Modal_Emotion_Recognition","https://www.researchgate.net/publication/224929717_Combining_Efforts_for_Improving_Automatic_Classification_of_Emotional_User_States","https://www.researchgate.net/publication/226199229_Real-Time_Inference_of_Complex_Mental_States_from_Facial_Expressions_and_Head_Gestures","https://www.researchgate.net/publication/220817465_Towards_Emotion_Prediction_in_Spoken_Tutoring_Dialogues","https://www.researchgate.net/publication/215835853_Toward_multimodal_fusion_of_affective_cues","https://www.researchgate.net/publication/2986190_Toward_an_affect-sensitive_multimodal_human-computer_interaction","https://www.researchgate.net/publication/224929651_Affect-Robust_Speech_Recognition_by_Dynamic_Emotional_Adaptation","https://www.researchgate.net/publication/221482287_Speaker_independent_emotion_recognition_by_early_fusion_of_acoustic_and_linguistic_features_within_ensembles","https://www.researchgate.net/publication/221485053_Spontaneous_speech_How_people_really_talk_and_why_engineers_should_care","https://www.researchgate.net/publication/224762178_Mothers_adults_children_pets_-_Towards_the_acoustics_of_intimacy","https://www.researchgate.net/publication/247130301_Perception_of_social_interest","https://www.researchgate.net/publication/221052632_The_painful_face_-_Pain_expression_recognition_using_active_appearance_models","https://www.researchgate.net/publication/221480335_Using_context_to_improve_emotion_detection_in_spoken_dialog_systems","https://www.researchgate.net/publication/3940582_Rapid_object_detection_using_a_boosted_cascade_of_simple_features_Proceedings_of_the_2001","https://www.researchgate.net/publication/224929701_Towards_responsive_Sensitive_Artificial_Listeners","https://www.researchgate.net/publication/4084629_Pitch-based_emphasis_detection_for_characterization_of_meeting_recordings","https://www.researchgate.net/publication/4027751_Hidden_Markov_model-based_speech_emotion_recognition","https://www.researchgate.net/publication/2379283_Text_Categorization_with_Support_Vector_Machines_Learning_with_Many_Relevant_Features","https://www.researchgate.net/publication/2455596_A_Real-Time_Filled_Pause_Detection_System_For_Spontaneous_Speech_Recognition","https://www.researchgate.net/publication/220017784_Data_Mining_Pratical_Machine_Learning_Tools_and_Techniques","https://www.researchgate.net/publication/221053816_RealTourist_-_A_Study_of_Augmenting_Human-Human_and_Human-Computer_Dialogue_with_Eye-Gaze_Overlay","https://www.researchgate.net/publication/36421263_A_System_for_Automatic_Face_Analysis_Based_on_Statistical_Shape_and_Texture_Models","https://www.researchgate.net/publication/29470491_Spontaneous_and_reflexive_eye_activity_measures_of_mental_workload","https://www.researchgate.net/publication/221052708_Foundations_of_Human_Computing_Facial_Expression_and_Emotion","https://www.researchgate.net/publication/225940935_Facial_Expression_Analysis","https://www.researchgate.net/publication/220485206_Assessing_Agreement_on_Classification_Tasks_The_Kappa_Statistic","https://www.researchgate.net/publication/220716696_On_the_Use_of_NonVerbal_Speech_Sounds_in_Human_Communication","https://www.researchgate.net/publication/3996862_Towards_automation_of_usability_studies","https://www.researchgate.net/publication/4076202_An_automated_face_reader_for_fatigue_detection","https://www.researchgate.net/publication/2336311_The_Cu-Htk_March_2000_Hub5e_Transcription_System","https://www.researchgate.net/publication/2875967_Towards_Emotion_Prediction_in_Spoken_Tutoring_Dialogues","https://www.researchgate.net/publication/228605077_To_display_or_not_to_display_towards_the_architecture_of_a_reflexive_agent","https://www.researchgate.net/publication/224929710_Low-Level_Fusion_of_Audio_and_Video_Features_For_Multi-Modal_Emotion_Recognition","https://www.researchgate.net/publication/221487656_Automatic_laughter_detection_using_neural_networks","https://www.researchgate.net/publication/234837419_Facial_and_Vocal_Expressions_of_Emotion","https://www.researchgate.net/publication/222431291_Emotional_speech_recognition_Resources_features_and_methods","https://www.researchgate.net/publication/215721756_Lecture_Notes_in_Computer_Science","https://www.researchgate.net/publication/7489451_Adaptive_active_appearance_models","https://www.researchgate.net/publication/232615384_Automated_Posture_Analysis_for_Detecting_Learners_Interest_Level","https://www.researchgate.net/publication/228776646_Additive_Logistic_Regression_A_Statistical_View_of_Boosting","https://www.researchgate.net/publication/221485864_Recognition_of_interest_in_human_conversational_speech","https://www.researchgate.net/publication/221489265_Processing_disfluent_speech_how_and_when_are_disfluencies_found","https://www.researchgate.net/publication/223009662_Automatic_recognition_and_analysis_of_human_faces_and_facial_expression_A_survey_Pattern_Recog","https://www.researchgate.net/publication/224929704_Abandoning_Emotion_Classes_--_Towards_Continuous_Emotion_Recognition_with_Modelling_of_Long-Range_Dependencies","https://www.researchgate.net/publication/4136873_Detecting_Group_Interest-Level_in_Meetings","https://www.researchgate.net/publication/220928181_Probabilistic_Combination_of_Multiple_Modalities_to_Detect_Interest","https://www.researchgate.net/publication/2984124_Rabiner_L_A_Tutorial_on_Hidden_Markov_Models_and_Selected_Applications_in_Speech_Recognition_Proc_IEEE_772_257-286","https://www.researchgate.net/publication/32887586_Modeling_Multimodal_Expression_of_User's_AffectiveSubjective_Experience","https://www.researchgate.net/publication/4119623_Real-Time_Inference_of_Complex_Mental_States_from_Facial_Expressions_and_Head_Gestures","https://www.researchgate.net/publication/224057442_Submotions_for_Hidden_Markov_Model_Based_Dynamic_Facial_Action_Recognition","https://www.researchgate.net/publication/3618363_Acoustic_and_language_modeling_of_human_and_nonhuman_noises_forhuman-to-human_spontaneous_speech_recognition","https://www.researchgate.net/publication/221487883_ANVIL_-_A_Generic_Annotation_Tool_for_Multimodal_Dialogue","https://www.researchgate.net/publication/221622170_On_the_Necessity_and_Feasibility_of_Detecting_a_Driver's_Emotional_State_While_Driving","https://www.researchgate.net/publication/5608691_Modeling_focus_of_attention_for_meeting_indexing_based_on_multiple_cues","https://www.researchgate.net/publication/220955196_Gaze-X_Adaptive_Affective_Multimodal_Interface_for_Single-User_Office_Scenarios","https://www.researchgate.net/publication/23493444_A_Survey_of_Affect_Recognition_Methods_Audio_Visual_and_Spontaneous_Expressions","https://www.researchgate.net/publication/239452767_Fixational_eye_movements_and_retinal_activity_during_a_single_visual_fixation","https://www.researchgate.net/publication/220726894_On_the_Influence_of_Phonetic_Content_Variation_for_Acoustic_Emotion_Recognition","https://www.researchgate.net/publication/4210761_Affect_recognition_from_face_and_body_early_fusion_vs_late_fusion","https://www.researchgate.net/publication/221514781_E-motional_advantage_Performance_and_satisfaction_gains_with_affective_computing","https://www.researchgate.net/publication/20813767_Perceptual_Linear_Predictive_PLP_Analysis_of_Speech","https://www.researchgate.net/publication/3193162_The_FERET_evaluation_methodology_for_face-recognition_algorithms_IEEE_Trans_Pattern_Anal_Mach_Intell","https://www.researchgate.net/publication/220622803_Aspekte_effizienten_Usability_Engineerings_Aspects_of_Efficient_Usability_Engineering"]}
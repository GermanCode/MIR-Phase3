{"id":273327856,"title":"Hamiltonian ABC","authors":["Edward Meeds","Robert Leenders","Max Welling"],"_abstract":"ABSTRACT Approximate Bayesian computation (ABC) is a powerful and elegant framework for performing inference in simulation-based models. However, due to the difficulty in scaling likelihood estimates, ABC remains useful for relatively low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of likelihood-free algorithms that apply recent advances in scaling Bayesian learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find that a small number forward simulations can effectively approximate the ABC gradient, allowing Hamiltonian dynamics to efficiently traverse parameter spaces. We also describe a new simple yet general approach of incorporating random seeds into the state of the Markov chain, further reducing the random walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and show that HABC samples comparably to regular Bayesian inference using true gradients on a high-dimensional problem from machine learning.","cited_in":[],"reference":["https://www.researchgate.net/publication/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","https://www.researchgate.net/publication/45630121_Statistical_inference_for_noisy_nonlinear_ecological_dynamic_systems","https://www.researchgate.net/publication/236653886_Approximate_Bayesian_Computation_ABC_gives_exact_results_under_the_assumption_of_model_error","https://www.researchgate.net/publication/259584383_Accelerating_ABC_methods_using_Gaussian_processes","https://www.researchgate.net/publication/269692923_Bayesian_Sampling_Using_Stochastic_Gradient_Thermostats","https://www.researchgate.net/publication/228926940_Gaussian_Processes_to_Speed_up_Hybrid_Monte_Carlo_for_Expensive_Bayesian_Integrals"]}
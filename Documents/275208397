{"id":275208397,"title":"Speaker-Independent Speech Emotion Recognition using Gaussian and SVM Classifiers,","authors":["Dr.Yousaf Khan","S. Haq","Amjad Ali","M. Asif","T. Jan","Naveedahmad"],"_abstract":"This paper presents a novel technique of human emotion recognition using the audio modality for the speaker-independent task. In order to achieve a high emotion classification performance a standard set of audio features were extracted. The feature selection was performed using the Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. The feature selection was followed by feature reduction using PCA and LDA, and classification using the Gaussian and SVM classifiers. The emotion classification performance better or comparable to state-of-the art techniques and humans were achieved on the standard Berlin emotional speech database. Keywords: Audio Emotion Recognition, Feature Selection, Principal Component Analysis, Gaussian and SVM Classifiers","cited_in":[],"reference":["https://www.researchgate.net/publication/232616387_Recognizing_Facial_Expression_Machine_Learning_and_Application_to_Spontaneous_Behavior","https://www.researchgate.net/publication/228787343_'You_stupid_tin_box'_-_Children_interacting_with_the_AIBO_robot_A_cross-linguistic_emotional_speech_corpus","https://www.researchgate.net/publication/221052478_Analysis_of_emotion_recognition_using_facial_expressions_speech_and_multimodal_information","https://www.researchgate.net/publication/222425087_Multi-Style_Classification_of_Speech_Under_Stress_Using_Feature_Subset_Selection_Based_on_Genetic_Algorithms","https://www.researchgate.net/publication/3321357_Emotion_recognition_in_human-computer_interaction_Signal_Process_Mag","https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit","https://www.researchgate.net/publication/4210761_Affect_recognition_from_face_and_body_early_fusion_vs_late_fusion","https://www.researchgate.net/publication/227296392_Multimodal_Emotion_Recognition_in_Speech-based_Interaction_Using_Facial_Expression_Body_Gesture_and_Acoustic_Analysis","https://www.researchgate.net/publication/224058228_Improvement_of_Emotion_Recognition_from_Voice_by_Separating_of_Obstruents","https://www.researchgate.net/publication/3334063_Toward_detecting_emotions_in_spoken_dialogs","https://www.researchgate.net/publication/221571403_Affective_multimodal_human-computer_interaction","https://www.researchgate.net/publication/224929671_The_Interspeech_2009_Emotion_Challenge","https://www.researchgate.net/publication/224929705_Patterns_Prototypes_Performance_Classifying_Emotional_User_States","https://www.researchgate.net/publication/221479136_Combining_frame_and_turn-level_information_for_robust_recognition_of_emotions_within_speech","https://www.researchgate.net/publication/224929718_Balancing_Spoken_Content_Adaptation_and_Unit_Length_in_the_Recognition_of_Emotion_and_Interest","https://www.researchgate.net/publication/23493444_A_Survey_of_Affect_Recognition_Methods_Audio_Visual_and_Spontaneous_Expressions"]}
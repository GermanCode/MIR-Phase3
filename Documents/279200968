{"id":279200968,"title":"Systematic Analysis of Video Data from Different Human-Robot Interaction Studies: A Categorisation of Social Signals During Error Situations","authors":["Manuel Giuliani","Nicole Mirnig","Gerald Stollnberger","Susanne Stadler","Roland Buchner","Manfred Tscheligi"],"_abstract":"ABSTRACT Human-robot interactions are often affected by error situations that are caused by either the robot or the human. Therefore, robots would profit from the ability to recognise when error situations occur. We investigated the verbal and non-verbal social signals that humans show when error situations occur in human-robot interaction experiments. For that, we analysed 201 videos of five human-robot interaction user studies with varying tasks from four independent projects. The analysis shows that there are two types of error situations: social norm violations and technical failures. Social norm violations are situations in which the robot does not adhere to the underlying social script of the interaction. Technical failures are caused by technical shortcomings of the robot. The results of the video analysis show that the study participants use many head movements and very few gestures, when in an error situation with the robot. We also found that the participants talked more in the case of social norm violations and less during technical failures. Finally, the participants use fewer non-verbal social signals (for example smiling, nodding, and head shaking), when they are interacting with the robot alone and no experimenter or other human is present. The results suggest that participants do not see the robot as a social interaction partner with comparable communication skills. Our findings have implications for builders and evaluators of human-robot interaction systems. The builders need to consider including modules for recognition and classification of head movements to the robot input channels. The evaluators need to make sure that the presence of an experimenter does not skew the results of their user studies.","cited_in":[{"id":285054097,"url":"https://www.researchgate.net/publication/285054097_Impact_of_Robot_Actions_on_Social_Signals_and_Reaction_Times_in_HRI_Error_Situations"}],"reference":["https://www.researchgate.net/publication/226077259_How_a_co-actor's_task_affects_monitoring_of_own_errors_Evidence_from_a_social_event-related_potential_study","https://www.researchgate.net/publication/209436215_An_Argument_For_Basic_Emotions","https://www.researchgate.net/publication/231182910_Two_People_Walk_Into_a_Bar_Dynamic_Multi-Party_Social_Interaction_with_a_Robot_Agent","https://www.researchgate.net/publication/257987094_Comparing_Task-based_and_Socially_Intelligent_Behaviour_in_a_Robot_Bartender","https://www.researchgate.net/publication/264546128_Updating_freeze_Aligning_animal_and_human_research","https://www.researchgate.net/publication/260658406_Exploring_Temporal_Patterns_in_Classifying_Frustrated_and_Delighted_Smiles","https://www.researchgate.net/publication/241621241_Natural_Problems_of_Naturalistic_Video_Data","https://www.researchgate.net/publication/235421597_Motion_as_a_cue_to_face_recognition_Evidence_from_congenital_prosopagnosia","https://www.researchgate.net/publication/246333495_Scripts_Plans_Goals_and_Understanding_An_Inquiry_Into_Human_Knowledge_Structures","https://www.researchgate.net/publication/273141507_I_Trained_this_robot_The_impact_of_pre-experience_and_execution_behavior_on_robot_teachers","https://www.researchgate.net/publication/228191024_Social_Norms_and_Social_Roles","https://www.researchgate.net/publication/279264661_The_Interactive_Urban_Robot_User-centered_development_and_final_field_trial_of_a_direction_requesting_robot","https://www.researchgate.net/publication/220613473_Human-Robot_Interaction_A_Survey","https://www.researchgate.net/publication/233722461_Integration_of_Error_Agency_and_Representation_of_Others'_Pain_in_the_Anterior_Insula","https://www.researchgate.net/publication/220605620_Toward_Humanlike_Task-Based_Dialogue_Processing_for_Human_Robot_Interaction","https://www.researchgate.net/publication/243690111_Human_Errors_A_Taxonomy_for_Describing_Human_Malfunction_in_Industrial_Installations","https://www.researchgate.net/publication/38135469_The_role_of_the_medial_frontal_cortex_in_cognitive_control","https://www.researchgate.net/publication/274712967_Robot_Pressure_The_Impact_of_Robot_Eye_Gaze_and_Lifelike_Bodily_Movements_upon_Decision-Making_and_Trust","https://www.researchgate.net/publication/229038870_Bridging_the_Gap_Between_Social_Animal_and_Unsocial_Machine_A_Survey_of_Social_Signal_Processing","https://www.researchgate.net/publication/263076720_What_does_not_happen_Quantifying_embodied_engagement_using_NIMI_and_self-adaptors","https://www.researchgate.net/publication/229059922_The_Repertoire_of_Nonverbal_Behavior_Categories_Origin_Usage_and_Coding","https://www.researchgate.net/publication/232459613_Pointing_and_placing","https://www.researchgate.net/publication/235416791_Social_neuroscience_The_second_phase","https://www.researchgate.net/publication/51057713_Mistakes_that_affect_others_An_fMRI_study_on_processing_of_own_errors_in_a_social_context","https://www.researchgate.net/publication/200788295_Situated_Reference_in_a_Hybrid_Human-Robot_Interaction_System","https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","https://www.researchgate.net/publication/261344847_Identifying_principal_social_signals_in_private_student-teacher_interactions_for_robot-enhanced_education","https://www.researchgate.net/publication/11245861_An_fMRI_study_of_intentional_and_unintentional_embarrassing_violations_of_social_norms","https://www.researchgate.net/publication/256452712_Automatic_detection_of_service_initiation_signals_used_in_bars","https://www.researchgate.net/publication/8231013_The_Role_of_the_Medial_Frontal_Cortex_in_Cognitive_Control","https://www.researchgate.net/publication/11332726_Recognizing_moving_faces_A_psychological_and_neural_synthesis","https://www.researchgate.net/publication/228663818_ELAN_A_professional_framework_for_multimodality_research"]}
{"id":220151284,"title":"Exploration–exploitation tradeoff using variance estimates in multi-armed bandits","authors":["Jean-Yves Audibert","Remi Munos","Csaba Szepesvári"],"_abstract":"ABSTRACT Algorithms based on upper confidence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, efficient and effective. This paper considers a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the different arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. We provide the first analysis of the expected regret for such algorithms. As expected, our results show that the algorithm that uses the variance estimates has a major advantage over its alternatives that do not use such estimates provided that the variances of the payoffs of the suboptimal arms are low. We also prove that the regret concentrates only at a polynomial rate. This holds for all the upper confidence bound based algorithms and for all bandit problems except those special ones where with probability one the payoff obtained by pulling the optimal arm is larger than the expected payoff for the second best arm. Hence, although upper confidence bound bandit algorithms achieve logarithmic expected regret rates, they might not be suitable for a risk-averse decision maker. We illustrate some of the results by computer simulations.","cited_in":[{"id":282844171,"url":"https://www.researchgate.net/publication/282844171_Context-Aware_Bandits"},{"id":282310775,"url":"https://www.researchgate.net/publication/282310775_Algorithms_for_Linear_Bandits_on_Polyhedral_Sets"},{"id":281670472,"url":"https://www.researchgate.net/publication/281670472_Asymptotically_Optimal_Multi-Armed_Bandit_Policies_under_a_Cost_Constraint"},{"id":282423266,"url":"https://www.researchgate.net/publication/282423266_Data-driven_robotic_sampling_for_marine_ecosystem_monitoring"},{"id":280589611,"url":"https://www.researchgate.net/publication/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking"},{"id":276149279,"url":"https://www.researchgate.net/publication/276149279_An_Asymptotically_Optimal_UCB_Policy_for_Uniform_Bandits_of_Unknown_Support"},{"id":272566466,"url":"https://www.researchgate.net/publication/272566466_JammingBandits_arXiv"},{"id":268227530,"url":"https://www.researchgate.net/publication/268227530_Jamming_Bandits"},{"id":270567764,"url":"https://www.researchgate.net/publication/270567764_The_Business_Management_Review_Volume_5_Number_3_Behavioural_segmentation_using_store_scanner_data_in_retailing_Exploration_and_exploitation_in_frequently_purchased_consumer_goods_markets"},{"id":278623534,"url":"https://www.researchgate.net/publication/278623534_Sub-sampling_for_Multi-armed_Bandits"},{"id":280046949,"url":"https://www.researchgate.net/publication/280046949_Nash_and_the_Bandit_Approach_for_Adversarial_Portfolios"},{"id":249314751,"url":"https://www.researchgate.net/publication/249314751_Linear_Bayes_Policy_for_Learning_in_Contextual-Bandits"},{"id":232416195,"url":"https://www.researchgate.net/publication/232416195_Adaptive_Stratified_Sampling_for_Monte-Carlo_integration_ofDifferentiable_functions"},{"id":224983695,"url":"https://www.researchgate.net/publication/224983695_Thompson_Sampling_An_Asymptotically_Optimal_Finite_Time_Analysis"},{"id":222942749,"url":"https://www.researchgate.net/publication/222942749_Knapsack_based_Optimal_Policies_for_Budget-Limited_Multi-Armed_Bandits"},{"id":221394148,"url":"https://www.researchgate.net/publication/221394148_Deviations_of_Stochastic_Bandit_Regret"},{"id":51918196,"url":"https://www.researchgate.net/publication/51918196_Robustness_of_Anytime_Bandit_Policies"},{"id":224246909,"url":"https://www.researchgate.net/publication/224246909_Test_error_bounds_for_classifiers_A_survey_of_old_and_new_results"},{"id":51891793,"url":"https://www.researchgate.net/publication/51891793_Stochastic_Bandit_Based_on_Empirical_Moments"},{"id":221345188,"url":"https://www.researchgate.net/publication/221345188_Fast_boosting_using_adversarial_bandits"},{"id":252185412,"url":"https://www.researchgate.net/publication/252185412_Self-bounding_functions_and_concentration_of_variance"},{"id":1737360,"url":"https://www.researchgate.net/publication/1737360_Pure_Exploration_for_Multi-Armed_Bandit_Problems"},{"id":220853222,"url":"https://www.researchgate.net/publication/220853222_Value_of_Learning_in_Sponsored_Search_Auctions"},{"id":225438563,"url":"https://www.researchgate.net/publication/225438563_Multi-armed_Bandits_with_Episode_Context"},{"id":220696313,"url":"https://www.researchgate.net/publication/220696313_Algorithms_for_Reinforcement_Learning"},{"id":220320997,"url":"https://www.researchgate.net/publication/220320997_Regret_Bounds_and_Minimax_Policies_under_Partial_Monitoring"},{"id":226908641,"url":"https://www.researchgate.net/publication/226908641_UCB_revisited_Improved_regret_bounds_for_the_stochastic_multi-armed_bandit_problem"},{"id":46585922,"url":"https://www.researchgate.net/publication/46585922_On_the_Doubt_about_Margin_Explanation_of_Boosting"},{"id":48209199,"url":"https://www.researchgate.net/publication/48209199_Online_Learning_of_Rested_and_Restless_Bandits"},{"id":220657509,"url":"https://www.researchgate.net/publication/220657509_Applying_Weak_Equivalence_of_Categories_Between_Partial_Map_and_Pointed_Set_Against_Changing_the_Condition_of_2-Arms_Bandit_Problem"},{"id":49688225,"url":"https://www.researchgate.net/publication/49688225_An_adaptive_and_robust_biological_network_based_on_the_vacant-particle_transportation_model"},{"id":222523650,"url":"https://www.researchgate.net/publication/222523650_Pure_exploration_in_finitely-armed_and_continuous-armed_bandits"},{"id":224243845,"url":"https://www.researchgate.net/publication/224243845_Sequential_learning_for_optimal_monitoring_of_multi-channel_wireless_networks"},{"id":51616491,"url":"https://www.researchgate.net/publication/51616491_BET_Bromodomain_Inhibition_as_a_Therapeutic_Strategy_to_Target_c-Myc"},{"id":220699078,"url":"https://www.researchgate.net/publication/220699078_ShareBoost_Boosting_for_Multi-view_Learning_with_Performance_Guarantees"},{"id":221393868,"url":"https://www.researchgate.net/publication/221393868_Upper-Confidence-Bound_Algorithms_for_Active_Learning_in_Multi-Armed_Bandits"},{"id":51949841,"url":"https://www.researchgate.net/publication/51949841_PAC-Bayesian_Inequalities_for_Martingales"},{"id":262274887,"url":"https://www.researchgate.net/publication/262274887_Optimistic_Bayesian_Sampling_in_Contextual-Bandit_Problems"},{"id":229438066,"url":"https://www.researchgate.net/publication/229438066_Meta-Learning_of_ExplorationExploitation_Strategies_The_Multi-ArmedBandit_Case"},{"id":260354186,"url":"https://www.researchgate.net/publication/260354186_In-Sample_and_Out-of-Sample_Model_Selection_and_Error_Estimation_for_Support_Vector_Machines"},{"id":262324696,"url":"https://www.researchgate.net/publication/262324696_Lower_Bounds_and_Selectivity_of_Weak-Consistent_Policies_in_Stochastic_Multi-Armed_Bandit_Problem"},{"id":267168309,"url":"https://www.researchgate.net/publication/267168309_Faster_Hoeffding_Racing_Bernstein_Races_via_Jackknife_Estimates"},{"id":273303733,"url":"https://www.researchgate.net/publication/273303733_A_Survey_of_old_and_New_Results_for_the_Test_Error_Estimation_of_a_Classifier"},{"id":268067029,"url":"https://www.researchgate.net/publication/268067029_Robust_Risk-averse_Stochastic_Multi-Armed_Bandits"},{"id":236240995,"url":"https://www.researchgate.net/publication/236240995_Exploration_and_Exploitation_of_Scratch_Games"},{"id":256606492,"url":"https://www.researchgate.net/publication/256606492_Concentration_inequalities_for_sampling_without_replacement"},{"id":268523654,"url":"https://www.researchgate.net/publication/268523654_Combining_the_advice_of_experts_with_randomized_boosting_for_robust_pattern_recognition"},{"id":257657122,"url":"https://www.researchgate.net/publication/257657122_Multi-Armed_Bandits_for_Intelligent_Tutoring_Systems"},{"id":262211234,"url":"https://www.researchgate.net/publication/262211234_Automatic_ad_format_selection_via_contextual_bandits"},{"id":257618584,"url":"https://www.researchgate.net/publication/257618584_Tune_and_mix_Learning_to_rank_using_ensembles_of_calibrated_multi-class_classifiers"},{"id":259288229,"url":"https://www.researchgate.net/publication/259288229_Relative_Upper_Confidence_Bound_for_the_K-Armed_Dueling_Bandit_Problem"},{"id":268172677,"url":"https://www.researchgate.net/publication/268172677_From_Bandits_to_Monte-Carlo_Tree_Search_The_Optimistic_Principle_Applied_to_Optimization_and_Planning"},{"id":262325262,"url":"https://www.researchgate.net/publication/262325262_Real-Time_Adaptation_of_Influence_Strategies_in_Online_Selling"},{"id":260013597,"url":"https://www.researchgate.net/publication/260013597_Robustness_of_stochastic_bandit_policies"},{"id":260003769,"url":"https://www.researchgate.net/publication/260003769_Online_Clustering_of_Bandits"},{"id":261141207,"url":"https://www.researchgate.net/publication/261141207_Modeling_Human_Decision_Making_in_Generalized_Gaussian_Multiarmed_Bandits"},{"id":262338111,"url":"https://www.researchgate.net/publication/262338111_Adaptive_Monte_Carlo_via_Bandit_Allocation"},{"id":268523655,"url":"https://www.researchgate.net/publication/268523655_Combining_Expert_Strategies_in_Multimodal_Classification"},{"id":264165443,"url":"https://www.researchgate.net/publication/264165443_The_use_of_Thompson_sampling_to_increase_estimation_precision"},{"id":264201342,"url":"https://www.researchgate.net/publication/264201342_Reward_Maximization_Justifies_the_Transition_from_Sensory_Selection_at_Childhood_to_Sensory_Integration_at_Adulthood"},{"id":266660265,"url":"https://www.researchgate.net/publication/266660265_Networked_bandits_with_disjoint_linear_payoffs"},{"id":263046503,"url":"https://www.researchgate.net/publication/263046503_Integrating_Monte_Carlo_Tree_Search_with_Knowledge-Based_Methods_to_Create_Engaging_Play_in_a_Commercial_Mobile_Game"},{"id":262526437,"url":"https://www.researchgate.net/publication/262526437_Dynamic_estimation_of_worker_reliability_in_crowdsourcing_for_regression_tasks_Making_it_work"},{"id":282741107,"url":"https://www.researchgate.net/publication/282741107_Satisficing_in_Gaussian_bandit_problems"},{"id":283202091,"url":"https://www.researchgate.net/publication/283202091_Accelerated_-Greedy_Multi_Armed_Bandit_Algorithm_for_Online_Sequential-Selections_Applications"},{"id":276906680,"url":"https://www.researchgate.net/publication/276906680_Advancing_E-Commerce_Personalization_Process_Framework_and_Case_Study"},{"id":272101762,"url":"https://www.researchgate.net/publication/272101762_Personalizing_persuasive_technologies_Explicit_and_implicit_personalization_using_persuasion_profiles"},{"id":282923929,"url":"https://www.researchgate.net/publication/282923929_Optimism-driven_exploration_for_nonlinear_systems"},{"id":280060683,"url":"https://www.researchgate.net/publication/280060683_Overtaking_Method_Based_on_Sand-Sifter_Mechanism_Why_do_Optimistic_Value_Functions_Find_Optimal_Solutions_in_Multi-Armed_Bandit_Problems"},{"id":280240021,"url":"https://www.researchgate.net/publication/280240021_Tutorial_on_Multi-Armed_Bandit_Problems_Models_and_Algorithms"},{"id":282978228,"url":"https://www.researchgate.net/publication/282978228_Multi-armed_bandit_approach_for_dynamic_pricing"},{"id":286920825,"url":"https://www.researchgate.net/publication/286920825_Continuing_Plan_Quality_Optimisation"},{"id":282976098,"url":"https://www.researchgate.net/publication/282976098_Approaches_to_Learning_to_Control_Dynamic_Uncertainty"},{"id":288059862,"url":"https://www.researchgate.net/publication/288059862_Satisficing_in_multi-armed_bandit_problems"}],"reference":["https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","https://www.researchgate.net/publication/38362521_On_Tail_Probabilities_for_Martingales","https://www.researchgate.net/publication/201976635_On_the_Likelihood_That_One_Unknown_Probability_Exceeds_Another_in_View_of_the_Evidence_of_Two_Samples","https://www.researchgate.net/publication/247441127_Sample_Mean_Based_Index_Policies_with_Olog_n_Regret_for_the_Multi-Armed_Bandit_Problem","https://www.researchgate.net/publication/221497328_Improved_Rates_for_the_Stochastic_Continuum-Armed_Bandit_Problem","https://www.researchgate.net/publication/221996154_Probability_Inequalities_For_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/228057769_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/3022289_Machine_Learning_and_Nonparametric_Bandit_Theory","https://www.researchgate.net/publication/221112399_Bandit_Based_Monte-Carlo_Planning","https://www.researchgate.net/publication/238378872_Modification_of_UCT_with_Patterns_in_Monte-Carlo_Go","https://www.researchgate.net/publication/247093571_PAC-Bayesian_statistical_learning_theory","https://www.researchgate.net/publication/239292007_Asymptotically_efficient_adaptive_allocation_rules1","https://www.researchgate.net/publication/201976486_Some_Aspects_of_the_Sequential_Design_of_Experiments"]}
{"id":281670476,"title":"Fast Second-Order Stochastic Backpropagation for Variational Inference","authors":["Kai Fan","Ziteng Wang","Jeff Beck","James Kwok","Katherine Heller"],"_abstract":"We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.","cited_in":[{"id":282181295,"url":"https://www.researchgate.net/publication/282181295_Deep_Temporal_Sigmoid_Belief_Networks_for_Sequence_Modeling"},{"id":288059688,"url":"https://www.researchgate.net/publication/288059688_High-Order_Stochastic_Gradient_Thermostats_for_Bayesian_Learning_of_Deep_Models"},{"id":288059869,"url":"https://www.researchgate.net/publication/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks"}],"reference":[]}
{"id":200777596,"title":"EmoVoice - A framework for online recognition of emotions from voice","authors":["Vogt","Thurid","Andrew Rosenberg","Elisabeth Andre","Nikolaus Bee","Nikolaus Bee"],"_abstract":"ABSTRACT We present EmoVoice, a framework for emotional speech corpus and classier creation and for oine as well as real-time online speech emotion recognition. The framework is intended to be used by non-experts and therefore comes with an interface to create an own personal or application specic emotion recogniser. Furthermore, we describe some applications and prototypes that already use our framework to track online emotional user states from voice information.","cited_in":[{"id":220746509,"url":"https://www.researchgate.net/publication/220746509_Wizard_of_Oz_Experiments_for_a_Companion_Dialogue_System_Eliciting_Companionable_Conversation"},{"id":221455070,"url":"https://www.researchgate.net/publication/221455070_How_was_your_day_A_companion_ECA"},{"id":224088139,"url":"https://www.researchgate.net/publication/224088139_Real-time_vocal_emotion_recognition_in_artistic_installations_and_interactive_storytelling_Experiences_and_lessons_learnt_from_CALLAS_and_IRIS"},{"id":224088188,"url":"https://www.researchgate.net/publication/224088188_Smart_sensor_integration_A_framework_for_multimodal_emotion_recognition_in_real-time"},{"id":221455864,"url":"https://www.researchgate.net/publication/221455864_Emotional_Input_for_Character-based_Interactive_Storytelling"},{"id":221479171,"url":"https://www.researchgate.net/publication/221479171_Exploring_the_benefits_of_discretization_of_acoustic_features_for_speech_emotion_recognition"},{"id":228973841,"url":"https://www.researchgate.net/publication/228973841_EmoEmma_Emotional_Speech_Input_for_Interactive_Storytelling_Demo_Paper"},{"id":228685634,"url":"https://www.researchgate.net/publication/228685634_Gaze_Behavior_during_Interaction_with_a_Virtual_Character_in_Interactive_Storytelling"},{"id":228296190,"url":"https://www.researchgate.net/publication/228296190_Close_Engagements_with_Artificial_Companions_Key_Social_Psychological_Ethical_and_Design_Issues"},{"id":229078352,"url":"https://www.researchgate.net/publication/229078352_Automatic_Detection_and_Classification_of_Prosodic_Events"},{"id":200777517,"url":"https://www.researchgate.net/publication/200777517_First_ideas_on_the_use_of_affective_cues_in_an_empathic_computer-based_companion"},{"id":251909556,"url":"https://www.researchgate.net/publication/251909556_Towards_user-independent_classification_of_multimodal_emotional_signals"},{"id":224088060,"url":"https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit"},{"id":271383379,"url":"https://www.researchgate.net/publication/271383379_Emotional_Sensitivity_in_Human-Computer_Interaction"},{"id":232620032,"url":"https://www.researchgate.net/publication/232620032_Evaluation_and_Discussion_of_Multi-modal_Emotion_Recognition"},{"id":220795824,"url":"https://www.researchgate.net/publication/220795824_EmoReSp_an_online_emotion_recognizer_based_on_speech"},{"id":221588547,"url":"https://www.researchgate.net/publication/221588547_Interaction_Strategies_for_an_Affective_Conversational_Agent"},{"id":221052464,"url":"https://www.researchgate.net/publication/221052464_Discovering_eye_gaze_behavior_during_human-agent_conversation_in_an_interactive_storytelling_application"},{"id":221485658,"url":"https://www.researchgate.net/publication/221485658_Age_and_gender_classification_from_speech_using_decision_level_fusion_and_ensemble_based_techniques"},{"id":221334098,"url":"https://www.researchgate.net/publication/221334098_The_Smart_Sensor_Integration_Framework_and_its_Application_in_EU_Projects"},{"id":220633913,"url":"https://www.researchgate.net/publication/220633913_An_Evaluation_of_Emotion_Units_and_Feature_Types_for_Real-Time_Speech_Emotion_Recognition"},{"id":222650661,"url":"https://www.researchgate.net/publication/222650661_Recognising_realistic_emotions_and_affect_in_speech_State_of_the_art_and_lessons_learnt_from_the_first_challenge"},{"id":261453737,"url":"https://www.researchgate.net/publication/261453737_Towards_a_Model_for_Recognising_the_Social_Attitude_in_Natural_Interaction_with_Embodied_Agents"},{"id":228677492,"url":"https://www.researchgate.net/publication/228677492_A_multimodal_corpus_for_gesture_expressivity_analysis"},{"id":257652279,"url":"https://www.researchgate.net/publication/257652279_NovA_Automated_Analysis_of_Nonverbal_Signals_in_Social_Interactions"}],"reference":["https://www.researchgate.net/publication/224060440_Playing_a_different_imitation_game_Interaction_with_an_Empathic_Android_Robot","https://www.researchgate.net/publication/221491017_A_database_of_German_emotional_speech","https://www.researchgate.net/publication/4181207_Comparing_Feature_Sets_for_Acted_and_Spontaneous_Speech_in_View_of_Automatic_Emotion_Recognition","https://www.researchgate.net/publication/224711604_Towards_More_Reality_in_the_Recognition_of_Emotional_Speech","https://www.researchgate.net/publication/224929717_Combining_Efforts_for_Improving_Automatic_Classification_of_Emotional_User_States","https://www.researchgate.net/publication/221484946_Integrating_information_from_speech_and_physiological_signals_to_achieve_emotional_sensitivity","https://www.researchgate.net/publication/225125163_Lecture_Notes_in_Computer_Science","https://www.researchgate.net/publication/228615684_Improving_automatic_emotion_recognition_from_speech_via_gender_differentiation","https://www.researchgate.net/publication/242636087_Data_Mining_Practical_Machine_Learning_Tools_with_Java_Implementations","https://www.researchgate.net/publication/221486724_Real_vs_acted_emotional_speech","https://www.researchgate.net/publication/228787343_'You_stupid_tin_box'_-_Children_interacting_with_the_AIBO_robot_A_cross-linguistic_emotional_speech_corpus","https://www.researchgate.net/publication/222417696_The_Production_and_Recognition_of_Emotions_in_Speech_Features_and_Algorithms","https://www.researchgate.net/publication/222967803_From_Greta's_mind_to_her_face_Modeling_the_dynamics_of_affective_states_in_a_Conversational_Embodied_Agent","https://www.researchgate.net/publication/254188446_An_Emotionally_Responsive_AR_Art_Installation","https://www.researchgate.net/publication/228619537_An_emotion-aware_voice_portal","https://www.researchgate.net/publication/235323874_Correlation-Based_Feature_Subset_Selection_for_Machine_Learning","https://www.researchgate.net/publication/2854468_The_SmartKom_Multimodal_Corpus_at_BAS","https://www.researchgate.net/publication/228715647_LIBSVM_A_library_for_support_vector_machines","https://www.researchgate.net/publication/226605838_Affective_Human-Robotic_Interaction","https://www.researchgate.net/publication/240685984_Acoustic_Emotion_Recognition_for_Affective_Computer_Gaming","https://www.researchgate.net/publication/200777595_Dancing_the_Night_Away_Controlling_a_Virtual_Karaoke_Dancer_by_Multimodal_Expressive_Cues","https://www.researchgate.net/publication/200777597_Affective_Interactive_Narrative_in_the_CALLAS_Project","https://www.researchgate.net/publication/222438726_A_Laboratory_Task_for_Induction_of_Mood_States","https://www.researchgate.net/publication/221151711_Developing_HMM-Based_Recognizers_with_ESMERALDA","https://www.researchgate.net/publication/237131440_PRAAT_Doing_phonetics_by_computer_Version_410_Computer_program","https://www.researchgate.net/publication/224929602_Effects_of_In-Car_Noise-Conditions_on_the_Recognition_of_Emotion_within_Speech","https://www.researchgate.net/publication/221478269_Using_system_and_user_performance_features_to_improve_emotion_detection_in_spoken_tutoring_dialogs"]}
{"id":221394179,"title":"Pure Exploration in Multi-armed Bandits Problems","authors":["SÃ©bastien Bubeck","Remi Munos","Gilles Stoltz"],"_abstract":"We consider the framework of stochastic multi-armed bandit prob- lems and study the possibilities and limitations of strategies that explore sequen- tially the arms. The strategies are assessed in terms of their simple regrets, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between simple and cumulative regrets. The main result is that the required exploration-exploitation trade-offs are qualitatively dif- ferent, in view of a general lower bound on the simple regret in terms of the cumulative regret.","cited_in":[{"id":281299068,"url":"https://www.researchgate.net/publication/281299068_Greedy_methods_randomization_approaches_and_multi-arm_bandit_algorithms_for_efficient_sparsity-constrained_optimization"},{"id":277924329,"url":"https://www.researchgate.net/publication/277924329_Qualitative_Multi-Armed_Bandits_A_Quantile-Based_Approach"},{"id":265967035,"url":"https://www.researchgate.net/publication/265967035_Best-Arm_Identification_in_Linear_Bandits"},{"id":280894846,"url":"https://www.researchgate.net/publication/280894846_Bandits_attack_function_optimization"},{"id":280046711,"url":"https://www.researchgate.net/publication/280046711_Differential_Evolution_Algorithm_Applied_to_Non-Stationary_Bandit_Problem"},{"id":262487352,"url":"https://www.researchgate.net/publication/262487352_Preference-Based_Rank_Elicitation_using_Statistical_Models_The_Case_of_Mallows"},{"id":262302672,"url":"https://www.researchgate.net/publication/262302672_Locally_Boosted_Graph_Aggregation_for_Community_Detection"},{"id":259718515,"url":"https://www.researchgate.net/publication/259718515_A_Boosting_Approach_to_Learning_Graph_Representations"},{"id":262487442,"url":"https://www.researchgate.net/publication/262487442_PAC_Rank_Elicitation_through_Adaptive_Sampling_of_Noisy_Preferences"},{"id":259478458,"url":"https://www.researchgate.net/publication/259478458_lil'_UCB_An_Optimal_Exploration_Algorithm_for_Multi-Armed_Bandits"},{"id":256082253,"url":"https://www.researchgate.net/publication/256082253_X-Armed_Bandits"},{"id":238951707,"url":"https://www.researchgate.net/publication/238951707_On_Finding_the_Largest_Mean_Among_Many"},{"id":259604341,"url":"https://www.researchgate.net/publication/259604341_Regret_Based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes"},{"id":233865906,"url":"https://www.researchgate.net/publication/233865906_Sequential_Testing_for_Sparse_Recovery"},{"id":234061312,"url":"https://www.researchgate.net/publication/234061312_Best_Arm_Identification_A_Unified_Approach_to_Fixed_Budget_and_Fixed_Confidence"},{"id":261344178,"url":"https://www.researchgate.net/publication/261344178_Strategic_Choices_Small_Budgets_and_Simple_Regret"},{"id":230802961,"url":"https://www.researchgate.net/publication/230802961_The_Sample_Complexity_of_Search_over_Multiple_Populations"},{"id":228095604,"url":"https://www.researchgate.net/publication/228095604_Parallelizing_Exploration-Exploitation_Tradeoffs_in_Gaussian_Process_Bandit_Optimization"},{"id":221664696,"url":"https://www.researchgate.net/publication/221664696_Finding_a_most_biased_coin_with_fewest_flips"},{"id":234061317,"url":"https://www.researchgate.net/publication/234061317_Multi-Bandit_Best_Arm_Identification"},{"id":224250577,"url":"https://www.researchgate.net/publication/224250577_Active_learning_for_personalizing_treatment"},{"id":229024205,"url":"https://www.researchgate.net/publication/229024205_X-Armed_Bandits"},{"id":48190924,"url":"https://www.researchgate.net/publication/48190924_Convergence_rates_of_efficient_global_optimization_algorithms"},{"id":42622516,"url":"https://www.researchgate.net/publication/42622516_Parameter_Tuning_by_Simple_Regret_Algorithms_and_Multiple_Simultaneous_Hypothesis_Testing"},{"id":221497434,"url":"https://www.researchgate.net/publication/221497434_Open_Loop_Optimistic_Planning"},{"id":48908111,"url":"https://www.researchgate.net/publication/48908111_Strategies_optimistes_en_apprentissage_par_renforcement"},{"id":48908025,"url":"https://www.researchgate.net/publication/48908025_Elements_pour_l'Apprentissage_et_l'Optimisation_de_Fonctions_Cheres"},{"id":220487900,"url":"https://www.researchgate.net/publication/220487900_Hedging_Strategies_for_Bayesian_Optimization"},{"id":228562278,"url":"https://www.researchgate.net/publication/228562278_Detail-free_Posted-Price_Mechanisms_for_Limited_Supply_Online_Auctions"},{"id":220320054,"url":"https://www.researchgate.net/publication/220320054_Regret_Bounds_for_Gaussian_Process_Bandit_Problems"},{"id":46587728,"url":"https://www.researchgate.net/publication/46587728_Portfolio_Allocation_for_Bayesian_Optimization"},{"id":225438563,"url":"https://www.researchgate.net/publication/225438563_Multi-armed_Bandits_with_Episode_Context"},{"id":221497549,"url":"https://www.researchgate.net/publication/221497549_Best_Arm_Identification_in_Multi-Armed_Bandits"},{"id":51932398,"url":"https://www.researchgate.net/publication/51932398_Dynamic_Pricing_with_Limited_Supply"},{"id":236155033,"url":"https://www.researchgate.net/publication/236155033_Hierarchical_Knowledge_Gradient_for_Sequential_Sampling"},{"id":224959372,"url":"https://www.researchgate.net/publication/224959372_Multiple_Identifications_in_Multi-Armed_Bandits"},{"id":224951990,"url":"https://www.researchgate.net/publication/224951990_Efficient_Bayes-Adaptive_Reinforcement_Learning_using_Sample-BasedSearch"},{"id":229438066,"url":"https://www.researchgate.net/publication/229438066_Meta-Learning_of_ExplorationExploitation_Strategies_The_Multi-ArmedBandit_Case"},{"id":230724449,"url":"https://www.researchgate.net/publication/230724449_Monte_Carlo_Search_Algorithm_Discovery_for_One_Player_Games"},{"id":236843599,"url":"https://www.researchgate.net/publication/236843599_Efficient_crowdsourcing_of_unknown_experts_using_multi-armed_bandits"},{"id":236195853,"url":"https://www.researchgate.net/publication/236195853_Learning_and_exploration_in_action-perception_loops"},{"id":236235241,"url":"https://www.researchgate.net/publication/236235241_Parallel_Gaussian_Process_Optimization_with_Upper_Confidence_Bound_and_Pure_Exploration"},{"id":236985261,"url":"https://www.researchgate.net/publication/236985261_Can_Small_Islands_Protect_Nearby_Coasts_From_Tsunamis_An_Active_Experimental_Design_Approach"},{"id":260303043,"url":"https://www.researchgate.net/publication/260303043_The_Sample_Complexity_of_Search_Over_Multiple_Populations"},{"id":262426404,"url":"https://www.researchgate.net/publication/262426404_Scalable_and_Efficient_Bayes-Adaptive_Reinforcement_Learning_Based_on_Monte-Carlo_Tree_Search"},{"id":280046733,"url":"https://www.researchgate.net/publication/280046733_Algorithm_Portfolios_for_Noisy_Optimization_Compare_Solvers_Early"},{"id":282570392,"url":"https://www.researchgate.net/publication/282570392_Sequential_Dynamic_Classification_for_Large_Scale_Multiclass_Problems"},{"id":283520854,"url":"https://www.researchgate.net/publication/283520854_Algorithm_Portfolios_for_Noisy_Optimization"},{"id":283354398,"url":"https://www.researchgate.net/publication/283354398_Online_Rank_Elicitation_for_Plackett-Luce_A_Dueling_Bandits_Approach"}],"reference":["https://www.researchgate.net/publication/229024205_X-Armed_Bandits","https://www.researchgate.net/publication/239060322_Combinatorial_Methods_in_Density_Estimation","https://www.researchgate.net/publication/1960873_Bandit_Algorithms_for_Tree_Search","https://www.researchgate.net/publication/239292007_Asymptotically_efficient_adaptive_allocation_rules1","https://www.researchgate.net/publication/221112399_Bandit_Based_Monte-Carlo_Planning","https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","https://www.researchgate.net/publication/220616937_The_Non-Stochastic_Multi-Armed_Bandit_Problem","https://www.researchgate.net/publication/221617828_Nearly_Tight_Bounds_for_the_Continuum-Armed_Bandit_Problem","https://www.researchgate.net/publication/2942622_The_Sample_Complexity_of_Exploration_in_the_Multi-Armed_Bandit_Problem","https://www.researchgate.net/publication/230595873_Convergence_of_Probability_Measures","https://www.researchgate.net/publication/225176417_PAC_Bounds_for_Multi-armed_Bandit_and_Markov_Decision_Processes","https://www.researchgate.net/publication/201976486_Some_Aspects_of_the_Sequential_Design_of_Experiments","https://www.researchgate.net/publication/236736804_On_the_method_of_bounded_differences_Surv_Comb","https://www.researchgate.net/publication/221497704_The_Budgeted_Multi-armed_Bandit_Problem","https://www.researchgate.net/publication/228057769_Probability_Inequalities_for_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/238378872_Modification_of_UCT_with_Patterns_in_Monte-Carlo_Go","https://www.researchgate.net/publication/45882775_Sharp_Dichotomies_for_Regret_Minimization_in_Metric_Spaces","https://www.researchgate.net/publication/221996154_Probability_Inequalities_For_Sums_of_Bounded_Random_Variables","https://www.researchgate.net/publication/237415931_Online_Optimization_inX_Armed_Bandits","https://www.researchgate.net/publication/221497549_Best_Arm_Identification_in_Multi-Armed_Bandits","https://www.researchgate.net/publication/5020980_ELEVEN_-_Tests_needed_for_a_Recommendation","https://www.researchgate.net/publication/221497401_Lower_Bounds_on_the_Sample_Complexity_of_Exploration_in_the_Multi-armed_Bandit_Problem"]}
{"id":288713780,"title":"Bridging the Gap between Stochastic Gradient MCMC and Stochastic Optimization","authors":["Changyou Chen","David Carlson","Zhe Gan","Chunyuan Li","Lawrence Carin"],"_abstract":"ABSTRACT Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian analogs to popular stochastic optimization methods; however, this connection is not well studied. We explore this relationship by applying simulated annealing to an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two key components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii) adaptive element-wise momentum weights. The zero-temperature limit gives a novel stochastic optimization method with adaptive element-wise momentum weights, while conventional optimization methods only have a shared, static momentum weight. Under certain assumptions, our theoretical analysis suggests the proposed simulated annealing approach converges close to the global optima. Experiments on several deep neural network models show state-of-the-art results compared to related stochastic optimization algorithms.","cited_in":[],"reference":["https://www.researchgate.net/publication/234140081_Reversible_Jump_MCMC_Simulated_Annealing_for_Neural_Networks","https://www.researchgate.net/publication/228671354_Large-scale_machine_learning_with_stochastic_gradient_descent_In_COMPSTAT","https://www.researchgate.net/publication/228095594_Modeling_Temporal_Dependencies_in_High-Dimensional_SequencesApplication_to_Polyphonic_Music_Generation_and_Transcription","https://www.researchgate.net/publication/260232379_Stochastic_Gradient_Hamiltonian_Monte_Carlo","https://www.researchgate.net/publication/262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation","https://www.researchgate.net/publication/272423025_RMSProp_and_equilibrated_adaptive_learning_rates_for_non-convex_optimization","https://www.researchgate.net/publication/269692923_Bayesian_Sampling_Using_Stochastic_Gradient_Thermostats","https://www.researchgate.net/publication/220320677_Adaptive_Subgradient_Methods_for_Online_Learning_and_Stochastic_Optimization","https://www.researchgate.net/publication/278244488_Scalable_Deep_Poisson_Factor_Analysis_for_Topic_Modeling","https://www.researchgate.net/publication/239582331_Stochastic_Relaxation_Gibbs_Distributions_and_the_Bayesian_Resoration_of_Images","https://www.researchgate.net/publication/227375782_Riemann_manifold_Langevin_and_Hamiltonian_Monte_Carlo_methods","https://www.researchgate.net/publication/216792705_What_is_the_Best_Multi-Stage_Architecture_for_Object_Recognition","https://www.researchgate.net/publication/269935079_Adam_A_Method_for_Stochastic_Optimization","https://www.researchgate.net/publication/220118677_Optimization_by_Simulated_Annealing","https://www.researchgate.net/publication/220564009_Hybrid_parallel_tempering_and_simulated_annealing_method","https://www.researchgate.net/publication/2306327_Annealed_Importance_Sampling","https://www.researchgate.net/publication/225291078_MCMC_using_Hamiltonian_dynamics","https://www.researchgate.net/publication/260340011_Scaling_Nonparametric_Bayesian_Inference_via_Subsample-Annealing","https://www.researchgate.net/publication/286123372_Stochastic_gradient_Riemannian_Langevin_dynamics_on_the_probability_simplex","https://www.researchgate.net/publication/229091480_Learning_Representations_by_Back_Propagating_Errors","https://www.researchgate.net/publication/243777012_Handbook_of_Analysis_and_Its_Foundations","https://www.researchgate.net/publication/286271944_On_the_importance_of_initialization_and_momentum_in_deep_learning","https://www.researchgate.net/publication/265295377_Consistency_and_fluctuations_for_stochastic_gradient_Langevin_dynamics","https://www.researchgate.net/publication/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","https://www.researchgate.net/publication/234131103_Stochastic_Pooling_for_Regularization_of_Deep_Convolutional_NeuralNetworks","https://www.researchgate.net/publication/233981807_ADADELTA_An_adaptive_learning_rate_method"]}
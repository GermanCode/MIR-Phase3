{"id":280590277,"title":"Optimally Confident UCB : Improved Regret for Finite-Armed Bandits","authors":["Tor Lattimore"],"_abstract":"I present the first algorithm for stochastic finite-armed bandits that simultaneously enjoys order-optimal problem-dependent regret and worst-case regret. The algorithm is based on UCB, but with a carefully chosen confidence parameter that optimally balances the risk of failing confidence intervals against the cost of excessive optimism. A brief empirical evaluation suggests the new algorithm is at least competitive with Thompson sampling.","cited_in":[{"id":284219230,"url":"https://www.researchgate.net/publication/284219230_Regret_Analysis_of_the_Finite-Horizon_Gittins_Index_Strategy_for_Multi-Armed_Bandits"}],"reference":["https://www.researchgate.net/publication/230627940_Improved_Algorithms_for_Linear_Stochastic_Bandits_extended_version","https://www.researchgate.net/publication/230859046_Further_Optimal_Regret_Bounds_for_Thompson_Sampling","https://www.researchgate.net/publication/51967820_Analysis_of_Thompson_Sampling_for_the_multi-armed_bandit_problem","https://www.researchgate.net/publication/221497329_Minimax_Policies_for_Adversarial_and_Stochastic_Bandits","https://www.researchgate.net/publication/221497549_Best_Arm_Identification_in_Multi-Armed_Bandits","https://www.researchgate.net/publication/29642836_Tuning_Bandit_Algorithms_in_Stochastic_Environments","https://www.researchgate.net/publication/226908641_UCB_revisited_Improved_regret_bounds_for_the_stochastic_multi-armed_bandit_problem","https://www.researchgate.net/publication/2265004_Gambling_in_a_rigged_casino_The_adversarial_multi-armed_bandit_problem","https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","https://www.researchgate.net/publication/224983572_Bandit_Theory_meets_Compressed_Sensing_for_high_dimensional_StochasticLinear_Bandit","https://www.researchgate.net/publication/266529330_An_Empirical_Evaluation_of_Thompson_Sampling","https://www.researchgate.net/publication/221497234_Stochastic_Linear_Optimization_under_Bandit_Feedback","https://www.researchgate.net/publication/225176417_PAC_Bounds_for_Multi-armed_Bandit_and_Markov_Decision_Processes","https://www.researchgate.net/publication/48207451_The_KL-UCB_Algorithm_for_Bounded_Stochastic_Bandits_and_Beyond","https://www.researchgate.net/publication/216300669_Bandit_Processes_and_Dynamic_Allocation_Indices_With_Discussion","https://www.researchgate.net/publication/259478458_lil'_UCB_An_Optimal_Exploration_Algorithm_for_Multi-Armed_Bandits","https://www.researchgate.net/publication/224983695_Thompson_Sampling_An_Asymptotically_Optimal_Finite_Time_Analysis","https://www.researchgate.net/publication/249011883_Thompson_Sampling_for_1-Dimensional_Exponential_Family_Bandits","https://www.researchgate.net/publication/239292007_Asymptotically_efficient_adaptive_allocation_rules1","https://www.researchgate.net/publication/221497401_Lower_Bounds_on_the_Sample_Complexity_of_Exploration_in_the_Multi-armed_Bandit_Problem","https://www.researchgate.net/publication/201976486_Some_Aspects_of_the_Sequential_Design_of_Experiments","https://www.researchgate.net/publication/23670997_Linearly_Parameterized_Bandits","https://www.researchgate.net/publication/201976635_On_the_Likelihood_That_One_Unknown_Probability_Exceeds_Another_in_View_of_the_Evidence_of_Two_Samples"]}
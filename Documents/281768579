{"id":281768579,"title":"On Accelerated Methods in Optimization","authors":["Andre Wibisono","Ashia C. Wilson"],"_abstract":"In convex optimization, there is an {\\em acceleration} phenomenon in which we can boost the convergence rate of certain gradient-based algorithms. We can observe this phenomenon in Nesterov's accelerated gradient descent, accelerated mirror descent, and accelerated cubic-regularized Newton's method, among others. In this paper, we show that the family of higher-order gradient methods in discrete time (generalizing gradient descent) corresponds to a family of first-order rescaled gradient flows in continuous time. On the other hand, the family of {\\em accelerated} higher-order gradient methods (generalizing accelerated mirror descent) corresponds to a family of second-order differential equations in continuous time, each of which is the Euler-Lagrange equation of a family of Lagrangian functionals. We also study the exponential variant of the Nesterov Lagrangian, which corresponds to a generalization of Nesterov's restart scheme and achieves a linear rate of convergence in discrete time. Finally, we show that the family of Lagrangians is closed under time dilation (an orbit under the action of speeding up time), which demonstrates the universality of this Lagrangian view of acceleration in optimization.","cited_in":[{"id":283658896,"url":"https://www.researchgate.net/publication/283658896_On_the_relationship_between_imitative_logit_dynamics_in_the_population_game_theory_and_mirror_descent_method_in_the_online_optimization_using_the_example_of_the_Shortest_Path_Problem"}],"reference":["https://www.researchgate.net/publication/2433873_Natural_Gradient_Works_Efficiently_in_Learning","https://www.researchgate.net/publication/280590551_The_proximal_distance_algorithm","https://www.researchgate.net/publication/257291640_A_method_of_solving_a_convex_programming_problem_with_convergence_rate_O1k2","https://www.researchgate.net/publication/243787569_Introductory_Lectures_on_Convex_Optimization_A_Basic_Course","https://www.researchgate.net/publication/23948415_Smooth_minimization_of_non-smooth_functions_Math_Program","https://www.researchgate.net/publication/225600217_Accelerating_the_cubic_regularization_of_Newton's_method_on_convex_problems","https://www.researchgate.net/publication/220589612_Cubic_regularization_of_Newton_method_and_its_global_performance","https://www.researchgate.net/publication/224285355_Adaptive_Restart_for_Accelerated_Gradient_Schemes","https://www.researchgate.net/publication/258114143_The_Information_Geometry_of_Mirror_Descent","https://www.researchgate.net/publication/273157680_A_Differential_Equation_for_Modeling_Nesterov's_Accelerated_Gradient_Method_Theory_and_Insights"]}
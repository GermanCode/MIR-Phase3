{"id":289587864,"title":"An Analysis of Rhythmic Staccato-Vocalization Based on Frequency Demodulation for Laughter Detection in Conversational Meetings","authors":["Sucheta Ghosh","Miloš Cerňak","Sarbani Palit","B. B. Chaudhuri"],"_abstract":"ABSTRACT Human laugh is able to convey various kinds of meanings in human communications. There exists various kinds of human laugh signal, for example: vocalized laugh and non vocalized laugh. Following the theories of psychology, among all the vocalized laugh type, rhythmic staccato-vocalization significantly evokes the positive responses in the interactions. In this paper we attempt to exploit this observation to detect human laugh occurrences, i.e., the laughter, in multiparty conversations from the AMI meeting corpus. First, we separate the high energy frames from speech, leaving out the low energy frames through power spectral density estimation. We borrow the algorithm of rhythm detection from the area of music analysis to use that on the high energy frames. Finally, we detect rhythmic laugh frames, analyzing the candidate rhythmic frames using statistics. This novel approach for detection of `positive' rhythmic human laughter performs better than the standard laughter classification baseline.","cited_in":[],"reference":["https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","https://www.researchgate.net/publication/11904369_Not_All_Laughs_are_Alike_Voiced_but_Not_Unvoiced_Laughter_Readily_Elicits_Positive_Affect","https://www.researchgate.net/publication/222415104_Automatic_discrimination_between_laughter_and_speech","https://www.researchgate.net/publication/228736939_Isochrony_and_prosodic_structure_in_British_English","https://www.researchgate.net/publication/276412348_Evaluation_From_precision_recall_and_F-measure_to_ROC_informedness_markedness_correlation","https://www.researchgate.net/publication/241455601_Positive_and_Negative_emotional_states_behind_the_l_aughs_in_spontaneous_spoken_dialogs","https://www.researchgate.net/publication/228715647_LIBSVM_A_library_for_support_vector_machines","https://www.researchgate.net/publication/224207538_Audiovisual_Discrimination_Between_Speech_and_Laughter_Why_and_When_Visual_Information_Might_Help","https://www.researchgate.net/publication/13790887_Tempo_and_beat_analysis_of_acoustic_music","https://www.researchgate.net/publication/259889921_The_INTERSPEECH_2013_computational_paralinguistics_challenge_Social_signals_conflict_emotion_autism","https://www.researchgate.net/publication/268925736_Rhythmic_Body_Movements_of_Laughter","https://www.researchgate.net/publication/46280888_Music_rhythm_rise_time_percepion_and_developmental_dyslexia_Perception_of_musical_meter_predicts_reading_and_phonology","https://www.researchgate.net/publication/262396204_Automatic_Detection_of_Laughter_and_Fillers_in_Spontaneous_Mobile_Phone_Conversations","https://www.researchgate.net/publication/244433556_LAUGHTER_DETECTION_IN_MEETINGS","https://www.researchgate.net/publication/259133420_Classification_of_social_laughter_in_natural_conversational_speech","https://www.researchgate.net/publication/4311149_Recognition_and_understanding_of_meetings_the_AMI_and_AMIDA_projects","https://www.researchgate.net/publication/202972390_A_Threshold_Selection_Method_from_Gray-Level_Histograms","https://www.researchgate.net/publication/224929655_openSMILE_--_The_Munich_Versatile_and_Fast_Open-Source_Audio_Feature_Extractor","https://www.researchgate.net/publication/7072669_Comparing_the_rhythm_and_melody_of_speech_and_music_The_case_of_British_English_and_French","https://www.researchgate.net/publication/228341280_The_AMI_meeting_corpus","https://www.researchgate.net/publication/220120772_Experimental_study_of_affect_bursts","https://www.researchgate.net/publication/279236940_Neurobiologic_Functions_of_Rhythm_Time_and_Pulse_in_Music","https://www.researchgate.net/publication/281492290_Laughter_and_Filler_Detection_in_Naturalistic_Audio","https://www.researchgate.net/publication/221487656_Automatic_laughter_detection_using_neural_networks"]}
{"id":221490308,"title":"INTERSPEECH 2007 The Relevance of Feature Type for the Automatic Classification of Emotional User States: Low Level Descriptors and Functionals","authors":["Björn Schuller","Anton Batliner","Dino Seppi","Stefan Steidl","Thurid Vogt","Johannes Wagner"],"_abstract":"ABSTRACT In this paper, we report on classification results for emotional user states (4 classes, German database of children interacting with a pet robot). Six sites computed acoustic and linguistic features independently from each other, following in part dif- ferent strategies. A total of 4244 features were pooled together and grouped into 12 low level descriptor types and 6 functional types. For each of these groups, classification results using Sup- port Vector Machines and Random Forests are reported for the full set of features, and for 150 features each with the highest individual Information Gain Ratio. The performance for the different groups varies mostly between ≈ 50% and ≈ 60%. Index Terms: emotional user states, automatic classification, feature types, functionals","cited_in":[{"id":282540984,"url":"https://www.researchgate.net/publication/282540984_The_Geneva_Minimalistic_Acoustic_Parameter_Set_GeMAPS_for_Voice_Research_and_Affective_Computing"},{"id":269291399,"url":"https://www.researchgate.net/publication/269291399_Non-linguistic_vocal_event_detection_using_online_random_forest"},{"id":274618827,"url":"https://www.researchgate.net/publication/274618827_Course_of_maternal_prosodic_incitation_motherese_during_early_development_in_autism_An_exploratory_home_movie_study"},{"id":257267505,"url":"https://www.researchgate.net/publication/257267505_Speaker_state_recognition_using_an_HMM-based_feature_extraction_method"},{"id":229438977,"url":"https://www.researchgate.net/publication/229438977_Toward_automating_a_human_behavioral_coding_system_for_married_couples'_interactions_using_speech_acoustic_features"},{"id":265797240,"url":"https://www.researchgate.net/publication/265797240_Personality_traits_detection_using_a_parallelized_modified_SFFS_algorithm"},{"id":220119847,"url":"https://www.researchgate.net/publication/220119847_Supervised_and_semi-supervised_infant-directed_speech_classification_for_parent-infant_interaction_analysis"},{"id":222650661,"url":"https://www.researchgate.net/publication/222650661_Recognising_realistic_emotions_and_affect_in_speech_State_of_the_art_and_lessons_learnt_from_the_first_challenge"},{"id":252057093,"url":"https://www.researchgate.net/publication/252057093_Feature_normalization_and_selection_for_robust_speaker_state_recognition"},{"id":224177560,"url":"https://www.researchgate.net/publication/224177560_A_Framework_for_Automatic_Human_Emotion_Classification_Using_Emotion_Profiles"},{"id":224186219,"url":"https://www.researchgate.net/publication/224186219_Automatic_Intonation_Recognition_for_the_Prosodic_Assessment_of_Language-Impaired_Children"},{"id":224207538,"url":"https://www.researchgate.net/publication/224207538_Audiovisual_Discrimination_Between_Speech_and_Laughter_Why_and_When_Visual_Information_Might_Help"},{"id":221292511,"url":"https://www.researchgate.net/publication/221292511_Benchmarking_classification_models_for_emotion_recognition_in_natural_speech_A_multi-corporal_study"},{"id":220655133,"url":"https://www.researchgate.net/publication/220655133_Automatic_Intonation_Recognition_for_the_Prosodic_Assessment_of_Language-Impaired_Children"},{"id":47791145,"url":"https://www.researchgate.net/publication/47791145_Detection_of_Clinical_Depression_in_Adolescents'_Speech_During_Family_Interactions"},{"id":229438937,"url":"https://www.researchgate.net/publication/229438937_A_new_multichannel_multimodal_dyadic_interaction_database"},{"id":228370109,"url":"https://www.researchgate.net/publication/228370109_Classification_of_affective_speech_using_normalized_time-frequency_cepstra"},{"id":224149743,"url":"https://www.researchgate.net/publication/224149743_Discrimination_of_speech_and_non-linguistic_vocalizations_by_Non-Negative_Matrix_Factorization"},{"id":221481003,"url":"https://www.researchgate.net/publication/221481003_A_new_multichannel_multi_modal_dyadic_interaction_database"},{"id":221483263,"url":"https://www.researchgate.net/publication/221483263_Automatic_classification_of_married_couples'_behavior_using_audio_features"},{"id":221490401,"url":"https://www.researchgate.net/publication/221490401_Determining_optimal_features_for_emotion_recognition_from_speech_by_applying_an_evolutionary_algorithm"},{"id":224114193,"url":"https://www.researchgate.net/publication/224114193_Emotional_speech_characterization_based_on_multi-features_fusion_for_face-to-face_interaction"},{"id":26656417,"url":"https://www.researchgate.net/publication/26656417_Acoustic_sleepiness_detection_Framework_and_validation_of_a_speech-adapted_pattern_recognition_approach"},{"id":224929707,"url":"https://www.researchgate.net/publication/224929707_Emotion_Recognition_from_Speech_Putting_ASR_in_the_Loop"},{"id":224375412,"url":"https://www.researchgate.net/publication/224375412_Motherese_Detection_Based_On_Segmental_and_Supra-Segmental_Features"},{"id":4363459,"url":"https://www.researchgate.net/publication/4363459_Emotion_sensitive_speech_control_for_human-robot_interaction_in_minimal_invasive_surgery"},{"id":224762178,"url":"https://www.researchgate.net/publication/224762178_Mothers_adults_children_pets_-_Towards_the_acoustics_of_intimacy"},{"id":220733194,"url":"https://www.researchgate.net/publication/220733194_Brute-forcing_hierarchical_functionals_for_paralinguistics_A_waste_of_feature_space"},{"id":224929600,"url":"https://www.researchgate.net/publication/224929600_Speaker_Noise_and_Acoustic_Space_Adaptation_for_Emotion_Recognition_in_the_Automotive_Environment"},{"id":224929704,"url":"https://www.researchgate.net/publication/224929704_Abandoning_Emotion_Classes_--_Towards_Continuous_Emotion_Recognition_with_Modelling_of_Long-Range_Dependencies"},{"id":221489506,"url":"https://www.researchgate.net/publication/221489506_Detection_of_security_related_affect_and_behaviour_in_passenger_transport"},{"id":224929705,"url":"https://www.researchgate.net/publication/224929705_Patterns_Prototypes_Performance_Classifying_Emotional_User_States"},{"id":224929694,"url":"https://www.researchgate.net/publication/224929694_Prosodic_and_Spectral_Features_within_Segment-based_Acoustic_Modeling"},{"id":242754241,"url":"https://www.researchgate.net/publication/242754241_Automatic_Motherese_Detection_for_Parent-Infant_Interaction"},{"id":242256468,"url":"https://www.researchgate.net/publication/242256468_Applied_to_Human_Centered_Interaction_Analysis"},{"id":267684984,"url":"https://www.researchgate.net/publication/267684984_Novel_Metrics_of_Speech_Rhythm_for_the_Assessment_of_Emotion"},{"id":257436125,"url":"https://www.researchgate.net/publication/257436125_Robust_emotion_recognition_in_noisy_speech_via_sparse_representation"},{"id":267859908,"url":"https://www.researchgate.net/publication/267859908_Speech_Production_Under_Cognitive_Load_Effects_and_Classification"},{"id":265047266,"url":"https://www.researchgate.net/publication/265047266_Creating_Human-centric_Expressive_Interfaces_Linking_Perceptual_evaluations_and_Engineering_Design_of_Synthetic_Multimodal_Communication"},{"id":13477947,"url":"https://www.researchgate.net/publication/13477947_Dietary_factors_influence_the_recovery_rates_of_Helicobacter_pylori_in_a_BALBcA_mouse_model"},{"id":221483320,"url":"https://www.researchgate.net/publication/221483320_Automatic_recognition_of_anger_in_spontaneous_speech"},{"id":221052199,"url":"https://www.researchgate.net/publication/221052199_Audiovisual_Laughter_Detection_Based_on_Temporal_Features"},{"id":221009711,"url":"https://www.researchgate.net/publication/221009711_An_Acoustic_Framework_for_Detecting_Fatigue_in_Speech_Based_Human-Computer-Interaction"},{"id":220716679,"url":"https://www.researchgate.net/publication/220716679_Automatic_Motherese_Detection_for_Face-to-Face_Interaction_Analysis"},{"id":220726872,"url":"https://www.researchgate.net/publication/220726872_Static_and_Dynamic_Modelling_for_the_Recognition_of_Non-verbal_Vocalisations_in_Conversational_Speech"},{"id":224929671,"url":"https://www.researchgate.net/publication/224929671_The_Interspeech_2009_Emotion_Challenge"},{"id":221523811,"url":"https://www.researchgate.net/publication/221523811_Time-Frequency_Features_Extraction_for_Infant_Directed_Speech_Discrimination"},{"id":254859818,"url":"https://www.researchgate.net/publication/254859818_How_does_real_affect_affect_affect_recognition_in_speech"},{"id":224375374,"url":"https://www.researchgate.net/publication/224375374_Multiple_Classifier_Applied_on_Predicting_Microsleep_from_Speech"},{"id":46294537,"url":"https://www.researchgate.net/publication/46294537_Time-Scale_Feature_Extractions_for_Emotional_Speech_Characterization"},{"id":224576207,"url":"https://www.researchgate.net/publication/224576207_Automatic_recognition_of_speech_emotion_using_long-term_spectro-temporal_features"},{"id":230556761,"url":"https://www.researchgate.net/publication/230556761_Emotion_Recognition_Using_a_Hierarchical_Binary_Decision_Tree_Approach"},{"id":224088137,"url":"https://www.researchgate.net/publication/224088137_The_hinterland_of_emotions_Facing_the_open-microphone_challenge"},{"id":224088139,"url":"https://www.researchgate.net/publication/224088139_Real-time_vocal_emotion_recognition_in_artistic_installations_and_interactive_storytelling_Experiences_and_lessons_learnt_from_CALLAS_and_IRIS"},{"id":224088060,"url":"https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit"},{"id":221478731,"url":"https://www.researchgate.net/publication/221478731_Acoustic_feature_analysis_in_speech_emotion_primitives_estimation"},{"id":221485244,"url":"https://www.researchgate.net/publication/221485244_A_cluster-profile_representation_of_emotion_using_agglomerative_hierarchical_clustering"},{"id":224929698,"url":"https://www.researchgate.net/publication/224929698_Word_Accent_and_Emotion"},{"id":220395374,"url":"https://www.researchgate.net/publication/220395374_Cross-Corpus_Acoustic_Emotion_Recognition_Variances_and_Strategies"},{"id":281042381,"url":"https://www.researchgate.net/publication/281042381_Towards_a_standard_set_of_acoustic_features_for_the_processing_of_emotion_in_speech"},{"id":42386822,"url":"https://www.researchgate.net/publication/42386822_Segmenting_into_Adequate_Units_for_Automatic_Recognition_of_Emotion-Related_Episodes_A_Speech-Based_Approach"},{"id":232638637,"url":"https://www.researchgate.net/publication/232638637_Cross-Corpus_Acoustic_Emotion_Recognition_Variances_and_Strategies"},{"id":221301421,"url":"https://www.researchgate.net/publication/221301421_Automatic_Speech-Based_Classification_of_Gender_Age_and_Accent"},{"id":221152292,"url":"https://www.researchgate.net/publication/221152292_Enhancing_Emotion_Recognition_from_Speech_through_Feature_Selection"},{"id":224206073,"url":"https://www.researchgate.net/publication/224206073_Automatic_classification_of_speaker_characteristics"},{"id":224929599,"url":"https://www.researchgate.net/publication/224929599_The_Automatic_Recognition_of_Emotions_in_Speech"},{"id":224929604,"url":"https://www.researchgate.net/publication/224929604_On_Laughter_and_Speech_Laugh_Based_on_Observations_of_Child-Robot_Interaction"},{"id":220121040,"url":"https://www.researchgate.net/publication/220121040_Application_of_speaker-_and_language_identification_state-of-the-art_techniques_for_emotion_recognition"},{"id":254903992,"url":"https://www.researchgate.net/publication/254903992_Automatic_stress_detection_in_emergency_telephone_calls"},{"id":224214743,"url":"https://www.researchgate.net/publication/224214743_Robust_representations_for_out-of-domain_emotions_using_Emotion_Profiles"},{"id":222820384,"url":"https://www.researchgate.net/publication/222820384_Whodunnit_-_Searching_for_the_Most_Important_Feature_Types_Signalling_Emotion-Related_User_States_in_Speech"},{"id":222646238,"url":"https://www.researchgate.net/publication/222646238_Detecting_emotional_state_of_a_child_in_a_conversational_computer_game"},{"id":51128668,"url":"https://www.researchgate.net/publication/51128668_Computerized_home_video_detection_for_motherese_may_help_to_study_impaired_interaction_between_infants_who_become_autistic_and_their_parents"},{"id":224238110,"url":"https://www.researchgate.net/publication/224238110_Obtaining_speech_assets_for_judgement_analysis_on_low-pass_filtered_emotional_speech"},{"id":220120082,"url":"https://www.researchgate.net/publication/220120082_Automatic_speech_emotion_recognition_using_modulation_spectral_features"},{"id":220633913,"url":"https://www.researchgate.net/publication/220633913_An_Evaluation_of_Emotion_Units_and_Feature_Types_for_Real-Time_Speech_Emotion_Recognition"},{"id":221622223,"url":"https://www.researchgate.net/publication/221622223_Affective_State_Recognition_in_Married_Couples'_Interactions_Using_PCA-Based_Vocal_Entrainment_Measures_with_Multiple_Instance_Learning"},{"id":220068793,"url":"https://www.researchgate.net/publication/220068793_Robust_emotion_recognition_by_spectro-temporal_modulation_statistic_features"},{"id":257513035,"url":"https://www.researchgate.net/publication/257513035_Features_and_classifiers_for_emotion_recognition_from_speech_a_survey_from_2000_to_2011"},{"id":254035496,"url":"https://www.researchgate.net/publication/254035496_Anger_recognition_in_Turkish_speech_using_acoustic_information"},{"id":255812106,"url":"https://www.researchgate.net/publication/255812106_Speech_information_retrieval_A_review"},{"id":233798284,"url":"https://www.researchgate.net/publication/233798284_Ooi_KE_Lech_M_Allen_NB_Multichannel_weighted_speech_classification_system_for_prediction_of_major_depression_in_adolescents_IEEE_Trans_Biomed_Eng_60_497-506"},{"id":261226051,"url":"https://www.researchgate.net/publication/261226051_Speech_based_Emotion_Recognition_based_on_hierarchical_decision_tree_with_SVM_BLG_and_SVR_classifiers"},{"id":257267503,"url":"https://www.researchgate.net/publication/257267503_On_the_development_of_an_automatic_voice_pleasantness_classification_and_intensity_estimation_system"},{"id":256613487,"url":"https://www.researchgate.net/publication/256613487_Behavioral_Signal_Processing_Deriving_Human_Behavioral_Informatics_From_Speech_and_Language"},{"id":257780597,"url":"https://www.researchgate.net/publication/257780597_Self-talk_Discrimination_in_Human-Robot_Interaction_Situations_for_Supporting_Social_Awareness"},{"id":257626910,"url":"https://www.researchgate.net/publication/257626910_Dimensionality_reduction-based_spoken_emotion_recognition"},{"id":236960130,"url":"https://www.researchgate.net/publication/236960130_On_the_Selection_of_Non-Invasive_Methods_Based_on_Speech_Analysis_Oriented_to_Automatic_Alzheimer_Disease_Diagnosis"},{"id":236644199,"url":"https://www.researchgate.net/publication/236644199_Do_Parentese_Prosody_and_Fathers'_Involvement_in_Interacting_Facilitate_Social_Interaction_in_Infants_Who_Later_Develop_Autism"},{"id":261208298,"url":"https://www.researchgate.net/publication/261208298_Classification_of_children_with_voice_impairments_using_deep_neural_networks"},{"id":261125002,"url":"https://www.researchgate.net/publication/261125002_Automatic_emotional_speech_recognition_in_Serbian_language"},{"id":259932023,"url":"https://www.researchgate.net/publication/259932023_Multimodal_Assistive_Technologies_for_Depression_Diagnosis_and_Monitoring"},{"id":271469117,"url":"https://www.researchgate.net/publication/271469117_Reduce_the_dimensions_of_emotional_features_by_principal_component_analysis_for_speech_emotion_recognition"},{"id":261339466,"url":"https://www.researchgate.net/publication/261339466_Classification_of_emotional_speech_units_in_call_centre_interactions"},{"id":269928011,"url":"https://www.researchgate.net/publication/269928011_Recognizing_emotion_from_Turkish_speech_using_acoustic_features"},{"id":277014082,"url":"https://www.researchgate.net/publication/277014082_A_Survey_Pre-processing_and_Feature_Extraction_Techniques_for_Depression_Analysis_Using_Speech_Signal"},{"id":282681315,"url":"https://www.researchgate.net/publication/282681315_Evaluation_of_syllable_rate_estimation_in_expressive_speech_and_its_contribution_to_emotion_recognition"},{"id":282979271,"url":"https://www.researchgate.net/publication/282979271_A_Short_Introduction_To_Laughter"},{"id":273277088,"url":"https://www.researchgate.net/publication/273277088_Within_and_cross-corpus_speech_emotion_recognition_using_latent_topic_model-based_features"}],"reference":["https://www.researchgate.net/publication/224711604_Towards_More_Reality_in_the_Recognition_of_Emotional_Speech","https://www.researchgate.net/publication/208032992_PRAAT_a_system_for_doing_phonetics_by_computer","https://www.researchgate.net/publication/224929717_Combining_Efforts_for_Improving_Automatic_Classification_of_Emotional_User_States","https://www.researchgate.net/publication/228863792_Of_all_things_the_measure_is_man_Automatic_classification_of_emotions_and_inter-labeler_consistency","https://www.researchgate.net/publication/221485330_Tales_of_tuning_-_prototyping_for_automatic_classification_of_emotional_user_states","https://www.researchgate.net/publication/220017784_Data_Mining_Pratical_Machine_Learning_Tools_and_Techniques","https://www.researchgate.net/publication/215721756_Lecture_Notes_in_Computer_Science","https://www.researchgate.net/publication/233967798_Development_of_a_Stemming_Algorithm","https://www.researchgate.net/publication/238651976_A_System_for_Doing_Phonetics_by_Computer","https://www.researchgate.net/publication/239506919_Lovins_JB_Development_of_a_Stemming_Algorithm_Mechanical_Translation_and_Computational_Linguistics_11_22-31"]}
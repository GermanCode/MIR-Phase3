{"id":262203674,"title":"AVEC 2012 - The continuous audio/visual emotion challenge - Aon","authors":["Bj√∂rn Schuller","Michel Valstar","Roddy Cowie","Maja Pantic"],"_abstract":"ABSTRACT The second international Audio/Visual Emotion Challenge and Workshop 2012 (AVEC 2012) is introduced shortly. 34 teams from 12 countries signed up for the Challenge. The SEMAINE database serves for prediction of four-dimensional continuous affect in audio and video. For the eligible participants, final scores for the Fully-Continuous Sub-Challenge ranged between a correlation coefficient between gold standard and prediction of 0.174 and 0.456, and for Word-Level Sub-Challenge between 0.113 and 0.280.","cited_in":[{"id":266654355,"url":"https://www.researchgate.net/publication/266654355_Correlated-spaces_regression_for_learning_continuous_emotion_dimensions"},{"id":236624251,"url":"https://www.researchgate.net/publication/236624251_Multiple_classifier_combination_using_reject_options_and_Markov_fusion_networks"},{"id":259891533,"url":"https://www.researchgate.net/publication/259891533_Facial_emotion_recognition_with_expression_energy"},{"id":262313338,"url":"https://www.researchgate.net/publication/262313338_Audio-visual_emotion_challenge_2012_A_simple_approach"},{"id":261271701,"url":"https://www.researchgate.net/publication/261271701_What_Really_Matters_A_Study_into_People's_Instinctive_Evaluation_Metrics_for_Continuous_Emotion_Prediction_in_Music"},{"id":262236861,"url":"https://www.researchgate.net/publication/262236861_Interactive_relevance_search_and_modeling_Support_for_expert-driven_analysis_of_multimodal_data"},{"id":267283342,"url":"https://www.researchgate.net/publication/267283342_Continuous_Conditional_Neural_Fields_for_Structured_Regression"},{"id":265300970,"url":"https://www.researchgate.net/publication/265300970_Human_Behavior_Understanding"},{"id":273396133,"url":"https://www.researchgate.net/publication/273396133_Vision_and_Attention_Theory_Based_Sampling_for_Continuous_Facial_Emotion_Recognition"},{"id":267502070,"url":"https://www.researchgate.net/publication/267502070_Automatic_Analysis_of_Facial_Affect_A_Survey_of_Registration_Representation_and_Recognition"},{"id":266561641,"url":"https://www.researchgate.net/publication/266561641_Representation_of_Facial_Expression_Categories_in_Continuous_Arousal-Valence_Space_Feature_and_Correlation"},{"id":271823672,"url":"https://www.researchgate.net/publication/271823672_Predicting_Mood_from_Punctual_Emotion_Annotations_on_Videos"},{"id":282320523,"url":"https://www.researchgate.net/publication/282320523_Semi-supervised_Kernel-Based_Temporal_Clustering"},{"id":275355521,"url":"https://www.researchgate.net/publication/275355521_Time-Delay_Neural_Network_for_Continuous_Emotional_Dimension_Prediction_From_Facial_Expression_Sequences"}],"reference":["https://www.researchgate.net/publication/224929655_openSMILE_--_The_Munich_Versatile_and_Fast_Open-Source_Audio_Feature_Extractor","https://www.researchgate.net/publication/221292323_String-based_audiovisual_fusion_of_behavioural_events_for_the_assessment_of_dimensional_affect","https://www.researchgate.net/publication/224248863_The_SEMAINE_Database_Annotated_Multimodal_Records_of_Emotionally_Colored_Conversations_between_a_Person_and_a_Limited_Agent","https://www.researchgate.net/publication/221292142_The_first_facial_expression_recognition_and_analysis_challenge","https://www.researchgate.net/publication/5816868_The_World_of_Emotions_is_not_Two-Dimensional","https://www.researchgate.net/publication/222574859_Facial_expression_recognition_based_on_Local_Binary_Patterns_A_comprehensive_study","https://www.researchgate.net/publication/224929671_The_Interspeech_2009_Emotion_Challenge","https://www.researchgate.net/publication/221481381_The_INTERSPEECH_2010_paralinguistic_challenge","https://www.researchgate.net/publication/221315112_Cost-Effective_Solution_to_Synchronized_Audio-Visual_Capture_Using_Multiple_Sensors","https://www.researchgate.net/publication/224238109_Emotion_representation_analysis_and_synthesis_in_continuous_space_A_survey","https://www.researchgate.net/publication/3193420_Multiresolution_Gray-Scale_and_Rotation_Invariant_Texture_Classification_with_Local_Binary_Patterns","https://www.researchgate.net/publication/228715647_LIBSVM_A_library_for_support_vector_machines","https://www.researchgate.net/publication/6690158_Face_Description_with_Local_Binary_Patterns_Application_to_Face_Recognition","https://www.researchgate.net/publication/221292325_Action_unit_detection_using_sparse_appearance_descriptors_in_space-time_video_volumes","https://www.researchgate.net/publication/221622197_AVEC_2011-The_First_International_AudioVisual_Emotion_Challenge","https://www.researchgate.net/publication/229059915_The_Sensitive_Artificial_Listener_an_induction_technique_for_generating_emotionally_coloured_conversation","https://www.researchgate.net/publication/209436026_'FEELTRACE'_An_instrument_for_recording_perceived_emotion_in_real_time","https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit"]}
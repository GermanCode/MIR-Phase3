{"id":282846166,"title":"Ensemble Nyström method for predicting conflict level from speech","authors":["Dong-Yan Huang","H. Li","Minghui Dong"],"_abstract":"ABSTRACT The Nyström method is an efficient technique for scaling kernel learning to very large data sets with more than millions. Instead of computing kernel matrix, it is to approximate a kernel learning problem with a linear prediction problem. We propose an ensemble Nyström method for high dimensional prediction of conflict level from speech. The experiments have been conducted over SSPNet Conflict Corpus, which contains 1430 clips of 30 seconds extracted from 45 political debates (roughly 12 hours of material). The results show that the proposed system is able to learn an optimal set of low-rank approximations and combine them to achieve higher accuracy of the prediction of conflict level. The correlation coefficient can be achieved to 84.94% between the actual and predicted labels, clearly exceeding the performance of the same regression using the original full feature set, as well as the state-of-the-art methods such as random k-nearest neighbor (RKNN) features (winning system in INTERSPEECH 2013 Computational Paralinguistics Challenge), support vector regressions, and regression techniques based on Gaussian Processes.","cited_in":[],"reference":["https://www.researchgate.net/publication/224877485_Speaker_State_Classification_based_on_Fusion_of_Asymmetric_SIMPLS_and_Support_Vector_Machines","https://www.researchgate.net/publication/3501836_The_Strength_of_Weak_Learnability","https://www.researchgate.net/publication/224929655_openSMILE_--_The_Munich_Versatile_and_Fast_Open-Source_Audio_Feature_Extractor","https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","https://www.researchgate.net/publication/248512106_Pattern_Recognition_and_Machine_Learning_Errata","https://www.researchgate.net/publication/221621725_Gaussian_Processes_in_Machine_Learning","https://www.researchgate.net/publication/51810217_Random_KNN_feature_selection_-_a_fast_and_stable_alternative_to_Random_Forests","https://www.researchgate.net/publication/278651717_The_Nature_of_Statistical_Learning_Theory","https://www.researchgate.net/publication/225625121_Geometry_on_Probability_Spaces","https://www.researchgate.net/publication/41166614_Sparse_partial_least_squares_for_simultaneous_dimension_reduction_and_variable_selection","https://www.researchgate.net/publication/3191841_Neural_Network_Ensembles","https://www.researchgate.net/publication/220344230_Measures_of_Diversity_in_Classifier_Ensembles_and_Their_Relationship_with_the_Ensemble_Accuracy","https://www.researchgate.net/publication/224586599_Implicit_Human-Centered_Tagging","https://www.researchgate.net/publication/221620135_Using_the_Nystrom_Method_to_Speed_Up_Kernel_Machines"]}
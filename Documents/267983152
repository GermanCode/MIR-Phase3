{"id":267983152,"title":"Sublinear Approximate Inference for Probabilistic Programs","authors":["Yutian Chen","Vikash K. Mansinghka","Zoubin Ghahramani"],"_abstract":"ABSTRACT Probabilistic programming languages can simplify the development of machine learning techniques, but only if inference is sufficiently scalable. Unfortunately, Bayesian parameter estimation for highly coupled models such as regressions and state-space models still scales badly. This paper describes a sublinear-time algorithm for making Metropolis-Hastings updates to latent variables in probabilistic programs. This approach generalizes recently introduced approximate MH techniques: instead of subsampling data items assumed to be independent, it subsamples edges in a dynamically constructed graphical model. It thus applies to a broader class of problems and interoperates with general-purpose inference techniques. Empirical results are presented for Bayesian logistic regression, nonlinear classification via joint Dirichlet process mixtures, and parameter estimation for stochastic volatility models (with state estimation via particle MCMC). All three applications use the same implementation, and each requires under 20 lines of probabilistic code.","cited_in":[],"reference":["https://www.researchgate.net/publication/262157385_Latent_Dirichlet_Allocation","https://www.researchgate.net/publication/261508268_Improving_Prediction_from_Dirichlet_Process_Mixtures_via_Enrichment","https://www.researchgate.net/publication/261289213_Venture_a_higher-order_probabilistic_programming_platform_with_programmable_inference","https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget","https://www.researchgate.net/publication/262350854_Monte_Carlo_MCMC_efficient_inference_by_approximate_sampling","https://www.researchgate.net/publication/229100346_Markov_Chain_Sampling_Methods_for_Dirichlet_Process_Mixture_Models"]}